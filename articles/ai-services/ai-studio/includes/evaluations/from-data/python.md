---
 title: include file
 description: include file
 author: eur
 ms.author: eric-urban
 ms.service: azure-ai-services
 ms.topic: include
 ms.date: 10/1/2023
 ms.custom: include
---

To thoroughly assess the performance of your generative AI application when applied to a substantial dataset, you can evaluate in your development environment with the Azure AI SDK. Given either a test dataset or flow target, your generative AI application performance will be quantitatively measured with both mathematical based metrics and AI-assisted metrics. This evaluation run will provide you with comprehensive insights into the application's capabilities and limitations. 

In this article you'll learn to create an evaluation run from a test dataset or flow with built-in evaluation metrics from Azure AI Studio SDK then view the results in Azure AI Studio if you choose to log it there. 

## Prerequisites

To evaluate with AI-assisted metrics, you need the following:

+ A test dataset in `.jsonl` format. See following section for dataset requirements
+ A deployment of one of these models: GPT 3.5 models, GPT 4 models, or Davinci models. Learn more about how to create a deployment [here](*TODO:REPLACEWITHDEPLOYMENTDOCLINK*).  

## Supported scenarios and datasets
We currently offer support for the following scenarios: 
+ **Question Answering**: This scenario is designed for applications that involve answering user queries and providing responses. 
+ **Conversation**: This scenario is suitable for applications where the model engages in conversation using a retrieval-augmented approach to extract information from your provided documents and generate detailed responses. 

For more in-depth information on each metric definition and how it is calculated, learn more [here](https://aka.ms/azureaistudioevaluationmetrics).

| Scenario           | Default metrics                          | All metrics                                                                                        |
|--------------------|------------------------------------------|----------------------------------------------------------------------------------------------------|
| Question Answering | Groundedness, Relevance, Coherence       | Groundedness, Relevance, Coherence, Fluency, GPT Similarity, F1 Score, Exact Match, ADA Similarity |
| Conversation       | Groundedness, Relevance, Retrieval Score | Groundedness, Relevance, Retrieval Score                                                           |

When using AI-assisted metrics for evaluation, you must specify a GPT model for the calculation process. Please choose a deployment with either GPT-3.5, GPT-4, or the Davinci model for your calculations. If you select ADA similarity, it requires an embedding model and you must additionally select a deployment featuring the `text-similarity-ada-001` or `text-similarity-ada-002` model to support ADA similarity calculations. 

### Supported input data format for Question Answering
We require question and answer pairs in `.jsonl` format with the required additional fields as follows:
| Metric         | Question      | Response      | Context       | Ground truth  |
|----------------|---------------|---------------|---------------|---------------|
| Groundedness   | Required: Str | Required: Str | Required: Str | N/A           |
| Relevance      | Required: Str | Required: Str | Required: Str | N/A           |
| Coherence      | Required: Str | Required: Str | N/A           | N/A           |
| Fluency        | Required: Str | Required: Str | N/A           | N/A           |
| GPT-similarity | Required: Str | Required: Str | N/A           | Required: Str |
| F1 Score | Required: Str | Required: Str | N/A           | Required: Str |
| Exact Match | Required: Str | Required: Str | N/A           | Required: Str |
| ADA similarity | Required: Str | Required: Str | N/A           | Required: Str |

- Question: the question asked by the user in Question Answer pair
- Response: the response to question generated by the model as answer
- Context: the source that response is generated with respect to (i.e. grounding documents)
- Ground truth: the response to question generated by user/human as the true answer

An example of a question and answer pair with context and ground truth provided:
```json
{"question":"What is the capital of France?",
"context":"France is in Europe",
"answer":"Paris is the capital of France.",
"ground_truth": "Paris"}
```
### Supported input data format for Conversation
We require a chat payload in the following `.jsonl` format which is a list of conversation turns in a conversation (below is called `"messages"`), for each conversation turn, it contains `content` which is the content of that turn of the conversation, `role` which is either the user or assistant and `"citations"` within `"context"` which provides the documents and its ID as key value pairs from the retrieval-augmented generation model. 

> **_NOTE:_** Currently only metrics for conversations or chat payloads with `"citations"` field provided (RAG scenario) are supported.

| Metric          | Citations from retrieved documents |
|-----------------|---------------------|
| Groundedness    | Required: str       |
| Relevance       | Required: str       |
| Retrieval score | Required: str       |

**Citations**: the relevant source from retrieved documents by retrieval model or user provided context that model's response is generated with respect to.

```json
{
    "messages": [
        {
            "content": "<conversation_turn_content>", 
            "role": "<role_name>", 
            "context": {
                "citations": [
                    {
                        "id": "<content_key>",
                        "content": "<content_value>"
                    }
                ]
            }
        }
    ]
}
```


## Evaluate with Azure AI SDK
Built-in evaluation metrics are available with the following installation
```python
pip install azure-ai-generative[evaluate]
```
Import default metrics with
```python
from azure.ai.generative.evaluate import evaluate
```
For the supported scenarios above, we provide default metrics by `task_type` as shown in the chart below. The `evaluate()` function calculates a default set of metrics with option to override metrics with `metrics_list` which accepts metrics as string:
| Scenario task type   | `task_type` value  | Default metrics | All metrics |
|------------------------------------------------------------|--------------------------------------------------------------------------------------|---|--------------------------------------------------------------------------------------------------------------------------|
| Question Answering                                         | `qa`              | `gpt_groundedness` (requires context), `gpt_relevance` (requires context), `gpt_coherence` | `gpt_groundedness`, `gpt_relevance`, `gpt_coherence`, `gpt_fluency`, `gpt_similarity`, `f1_score`, `exact_match`, `ada_similarity` |
| Single and multi-turn conversation (context required) | `chat`            |  `gpt_groundedness`, `gpt_relevance`, `gpt_retrieval_score`                                 |`gpt_groundedness`, `gpt_relevance`, `gpt_retrieval_score`                                 |
### Set up you Azure Open AI configurations for AI-assisted metrics
Before calling `evaluate()`, you need to set up your large language model deployment configuration in your environment which will be required for generating the AI-assisted metrics. 

```python
from azure.identity import DefaultAzureCredential
from azure.ai.generative import AIClient

client = AIClient.from_config(DefaultAzureCredential())
```
### Evaluate Question Answering: `qa`
#### Run a flow and evaluate
We provide an `evaluate` function call with the following interface for running a local flow then evaluating the results of those
```python
result = evaluate( 
    evaluation_name="my-qa-eval-with-flow", #name your evaluation to view in AI studio
    target=myflow, # pass in a flow that you want to run then evaluate results on 
    data=mydata, # data to be evaluated
    task_type="qa", # for different task types, different metrics are available
    metrics_list=["gpt_groundedness","gpt_relevance","gpt_coherence","gpt_fluency","gpt_similarity"] #optional superset over default set of metrics
    model_config= { #for AI-assisted metrics, need to hook up AOAI GPT model for doing the measurement
            "api_version": "2023-05-15",
            "api_base": os.getenv("OPENAI_API_BASE"),
            "api_type": "azure",
            "api_key": os.getenv("OPENAI_API_KEY"),
            "deployment_id": os.getenv("AZURE_OPENAI_EVALUATION_DEPLOYMENT")
    },
    data_mapping={
        "questions":"question", #column of data providing input to model
        "contexts":"context", #column of data providing context for each input
        "y_test":"groundtruth" #column of data providing ground truth answer, optional for default metrics
        },
    output_path="./myevalresults", #optional: save output artifacts to local folder path 
    tracking=client.tracking_uri #optional: if configured with AI client, evaluation gets logged to AI Studio
)
```

#### Evaluate on test dataset
Alternatively if you already have a test dataset and *do not need to run a flow* to get the generated results, you can alter the above function call to not take in a `target` parameter. However, if no `target` is specified, you must provide `"y_pred"` in your `data_mapping` parameter.

```python
result = evaluate( 
    evaluation_name="my-qa-eval-with-data", #name your evaluation to view in AI studio
    data=mydata, # data to be evaluated
    task_type="qa", # for different task types, different metrics are available
    metrics_list=["gpt_groundedness","gpt_relevance","gpt_coherence","gpt_fluency","gpt_similarity"] #optional superset over default set of metrics
    model_config= { #for AI-assisted metrics, need to hook up AOAI GPT model for doing the measurement
            "api_version": "2023-05-15",
            "api_base": os.getenv("OPENAI_API_BASE"),
            "api_type": "azure",
            "api_key": os.getenv("OPENAI_API_KEY"),
            "deployment_id": os.getenv("AZURE_OPENAI_EVALUATION_DEPLOYMENT")
    },
    data_mapping={
        "questions":"question", #column of data providing input to model
        "contexts":"context", #column of data providing context for each input
        "y_pred":"answer", #column of data providing output from model
        "y_test":"groundtruth" #column of data providing ground truth answer, optional for default metrics
        },
    output_path="./myevalresults", #optional: save evaluation results .jsonl to local folder path 
    tracking=client.tracking_uri #optional: if configured with AI client, evaluation gets logged to AI Studio
)
```
#### Evaluation result 
`Evaluate()` will output an `EvaluationResult()` that includes a `metric_summary` and `artifacts`.

```json
metric_summary = {"mean_gpt_groundedness":4.8, "mean_gpt_relevance":3.7, "mean_gpt_coherence":4.1}
metric_artifacts =
{"question":"What is the capital of France?",
"context":"France is in Europe",
"answer":"Paris is the capital of France.",
"ground_truth": "Paris",
"gpt_groundedness":"5","gpt_coherence":"5","gpt_relevance":"5"}
```
You can download your `EvaluationResult()` with the following
```python
result.download_evaluation_artifacts("./myevalresults")
```
### Evaluate Conversation: `chat`
The same interface can be used with `evaluate()` for the conversation scenario but with data mapping required only for model output `y_pred` and `task_type="chat"` shown below
```python
task_type="chat",
data_mapping={
        "y_pred":"messages", #key name of chat payload format that corresponds to each turn of the conversation if data with model generated output is provided without a target
        }
```

An example of an output: 
```json
{
  "messages": [
    {
      "content": "will my compass work in patagonia",
      "role": "user",
      "context": {
        "customer_info": "## customer_info\n  \nname: Jane Doe   \nage: 28    \nphone_number: 555-987-6543     item_number: 7 \n \n# chat history: \n \n# product context: \n\n"
      }
    },
    {
      "content": "Yes, the Pathfinder Pro-1 Adventure Compass can be used in Patagonia. It is designed for use in both the northern and southern hemispheres and has a built-in adjustable declination correction for precise navigation.",
      "role": "assistant",
      "session_state": {
        "id": "1234-5678-9012-3456"
      },
      "context": {
        "citations": [
          {
            "id": "data/3-product-info/product_info_66.md",
            "content": "# Information about product item_number: 66\nPathfinder Pro-1 Adventure Compass, priced: $39.99.\nExperience accurate navigation and superior outdoor performance\n\nFeatures:\n\nRobust, lightweight ABS plastic construction for durability and easy carry\nPrecision\nTechnical Specs:\n\nBest Use: Hiking, Orienteering, Backpacking"
          },
          {
            "id": "data/3-product-info/product_info_17.md",
            "content": "# Information about product item_number: 17\nRainGuard Hiking Jacket, price $110,\n\n## Brand\nMountainStyle\n\n## Category\nHiking Clothing\n## Features\nHere is a feature list for the RainGuard Hiking Jacket:\n## Features\n- Waterproof and Breathable: The RainGuard Hiking Jacket is designed to keep you dry and comfortable in wet conditions, thanks to its waterproof and breathable fabric.\n\n## Technical Specs\n- **Best Use:** Hiking"
          },
          {
            "id": "customer_info_and_recent_purchases",
            "content": "# current date_time: 2023-10-13 15:00:00\n\n## Customer Details:\n\nName: Jane Doe\nAge: 28\nPhone Number: 555-987-6543\n\n## Recent Purchases:\n\nOrder Number: 5\nDate: 2023-05-01\nItem:\n- Description: TrailMaster X4 Tent\n  Quantity: 1\n  Price: $250\n  Item Number: 1"
          }
        ]
      }
    },
    {
      "content": "and what is the temperature rating of my sleeping bag?",
      "role": "user",
      "session_state": {
        "id": "1234-5678-9012-3456"
      },
      "context": {
        "customer_info": "## customer_info\n  \nname: Jane Doe   \nage: 28 "
      }
    },
    {
      "content": "The CozyNights Sleeping Bag has a temperature rating of 20°F to 60°F (-6°C to 15°C). It is suitable for use in spring, summer, and fall seasons",
      "role": "assistant",
      "context": {
        "citations": [
          {
            "id": "data/3-product-info/product_info_7.md",
            "content": "# Information about product item_number: 7\nCozyNights Sleeping Bag, price $100,\n\n## Brand\nCozyNights\n\n## Category\nSleeping Bags\n\n## Features\n- **Lightweight**: Designed to be lightweight for easy carrying during outdoor adventures.\n- **3-Season**: Suitable for use in spring, summer, and fall seasons.\n\n## Technical Specifications\n- **Material**: Polyester\n- **Color**: Red\n\n## FAQ\n31) What is the temperature rating of the CozyNights Sleeping Bag?\nThe CozyNights Sleeping Bag is rated for 3-season use and has a temperature rating of 20 F to 60 F (-6 C to 15 C)."
          },
          {
            "id": "data/3-product-info/product_info_1.md",
            "content": "# Information about product item_number: 1\nTrailMaster X4 Tent, price $250,\n\n## Brand\nOutdoorLiving\n\n## Category\nTents\n\n## Features\n- Polyester material for durability\n- Spacious interior to accommodate multiple people\n\n## Technical Specs\n**Best Use**: Camping\n**Capacity**: 4-person"
          },
          {
            "id": "customer_info_and_recent_purchases",
            "content": "# current date_time: 2023-10-13 15:01:03\n\n## Customer Details:\n\nName: Jane Doe\nAge: 28\nPhone Number: 555-987-6543\nEmail: Jane.Doe@example.com\nAddress: 789 Broadway St, Seattle, WA 98101\n\nLoyalty Program: True\nLoyalty Program Level: Bronze\n\n## Recent Purchases:\n\nOrder Number: 5\nDate: 2023-05-01\nItem:\n- Description: TrailMaster X4 Tent\n  Quantity: 1\n  Price: $250\n  Item Number: 1"
          }
        ]
      }
    },
    {
      "content": "Awesome, thanks!",
      "role": "user",
      "session_state": {
        "id": "1234-5678-9012-3456"
      },
      "context": {
        "customer_info": "## customer_info\n  \nname: Jane Doe   \nage: 28    \nphone_number: 555-987-6543 "
      }
    },
    {
      "content": "You are welcome, Jane!",
      "role": "assistant",
      "context": {
        "citations": [
          {
            "id": "customer_info_and_recent_purchases",
            "content": "# current date_time: 2023-10-13 15:01:43\n\n## Customer Details:\n\nName: Jane Doe\nAge: 28\nPhone Number: 555-987-6543\nEmail: Jane.Doe@example.com\nAddress: 789 Broadway St, Seattle, WA 98101\n\nLoyalty Program: True\nLoyalty Program Level: Bronze\n\n## Recent Purchases:\n\nOrder Number: 5\nDate: 2023-05-01\nItem:\n- Description: TrailMaster X4 Tent\n  Quantity: 1\n  Price: $250\n  Item Number: 1"
          }
        ]
      }
    }
  ],
  "gpt_retrieval_score": 2,
  "gpt_groundedness": 3.6666666667,
  "gpt_relevance": 3.6666666667
}
```


## Next steps
 
If you logged your metrics to your AI studio project, you can locate the submitted evaluation run and its results within the run list by navigating to the 'Evaluation' tab of your Azure AI Studio project. Next, learn more about how to [view your evaluation results and metrics](*TODO:REPLACEWITHRELATIVELINK*).

Learn more about how to evaluate your generative AI applications:
+ [Evaluate your generative AI apps via the playground](https://aka.ms/evaluateplayground)
+ [Monitor your generative AI app in production](https://aka.ms/azureaistudiomonitoring)
 
Learn more about the [supported task types and built-in metrics](https://aka.ms/azureaistudioevaluationmetrics) and  [harm mitigation techniques](https://aka.ms/azureaistudioharmsmitigations).
