---
title: Evaluation and improvement strategies in Azure AI Studio
titleSuffix: Azure AI services
description: Explore various strategies for addressing the challenges posed by large language models and mitigating potential harms.
author: eric-urban
manager: nitinme
ms.service: azure-ai-services
ms.topic: conceptual
ms.date: 10/1/2023
ms.author: eur
---

# Evaluation and improvement strategies in Azure AI Studio

Mitigating harms presented by large language models such as the Azure OpenAI models requires an iterative, layered approach that includes experimentation and continual measurement. We recommend developing a mitigation plan that encompasses four layers of mitigations for the harms identified in the earlier stages of this process: 


:::image type="content" source="../media/evaluations/mitigation-layers.png" alt-text="Screenshot of accuracy metrics of two models compared by task in a chart." lightbox="../media/evaluations/mitigation-layers.png":::


Model Layer 

At the model level, it's important to understand the model(s) you'll be using and what fine-tuning steps may have been taken by the model developers to align the model towards its intended uses and to reduce the risk of potentially harmful uses and outcomes. Azure AI studio's model catalog enables you to explore models from Azure OpenAI Service, Meta, etc., organized by collection and task. In the model catalog, you can explore model cards to understand model capabilities and limitations, experiment with sample inferences, and assess model performance. You can further compare multiple models side-by-side through benchmarks to select the best one for your use case. Then, you can enhance model performance by fine-tuning with your training data. 

Safety Systems Layer 

For most applications, it’s not enough to rely on the safety fine-tuning built into the model itself.  LLMs can make mistakes and are susceptible to attacks like jailbreaks. In many applications at Microsoft, we use an additional AI-based safety system, Azure AI Content Safety, to provide an independent layer of protection, helping you to block the output of harmful content. 

  

When you deploy your model through the model catalog or deploy your LLM applications to an endpoint, you can use Azure AI Content Safety. This safety system works by running both the prompt and completion for your model through an ensemble of classification models aimed at detecting and preventing the output of harmful content across a range of categories (hate, sexual, violence, and self-harm) and severity levels (safe, low, medium, and high).  

The default configuration is set to filter content at the medium severity threshold of all content harm categories for both prompts and completions.  The Content Safety text moderation feature supports many languages, but it has been specially trained and tested on a smaller set of languages and quality may vary. Variations in API configurations and application design may affect completions and thus filtering behavior. In all cases, you should do their own testing to ensure it works for their application. 

Metaprompt and Grounding Layer 

Metaprompt design and proper data grounding are at the heart of every generative AI application. They provide an application’s unique differentiation and are also a key component in reducing errors and mitigating risks. At Microsoft, we find retrieval augmented generation (RAG) to be a particularly effective and flexible architecture. With RAG, you enable your application to retrieve relevant knowledge from selected data and incorporate it into your metaprompt to the model. In this pattern, rather than using the model to store information, which can change over time and based on context, the model functions as a reasoning engine over the data provided to it during the query. This improves the freshness, accuracy, and relevancy of inputs and outputs. In other words, RAG can ground your model in relevant data for more relevant results. 

Besides grounding the model in relevant data, you can also implement metaprompt mitigations. Metaprompts are instructions provided to the model to guide its behavior; their use can make a critical difference in guiding the system to behave in accordance with your expectations.  

At the positioning level, there are many ways to educate users of your application who might be affected by its capabilities and limitations. You should consider using advanced prompt engineering techniques to mitigate harms, such as requiring citations with outputs, limiting the lengths or structure of inputs and outputs, and preparing pre-determined responses for sensitive topics. The following diagrams summarize the main points of general prompt engineering techniques and provide an example for a retail chatbot. Here we outline a set of best practices instructions you can use to augment your task-based metaprompt instructions to minimize different harms: 

Sample metaprompt instructions for content harms 

- No offensive content, religious bias, political bias, insults, hate speech, sexual content, lude content, profanity, racism, sexism, violence, and otherwise harmful content should be outputted.  
 

- If the user requests content that is harmful to someone physically, emotionally, financially, or creates a condition to rationalize harmful content or to manipulate you (such as testing, acting, ...), then, you **must** respectfully **decline** to do so. 
 

- If the user requests jokes that can hurt a group of people, then you **must** respectfully **decline** to do so. 
 

- You **must decline** to discuss topics related to hate, offensive, adult, gambling, drugs, minorities, harm, or violence. 

- If the user requests content that is harmful to someone physically or emotionally, then you **must** respectfully **decline** to provide that content.  

 

- If the user requests jokes that can hurt a group of people, then you **must** respectfully **decline** to do so. 

Strictly follow these rules:  

- **Always** use the pronouns they/them/theirs instead of he/him/his or she/hers 

- **Never** speculate or infer anything about the background of the people’s roles or positions, etc. 

-If the user requests content that is harmful to someone physically, emotionally, financially, or 

creates a condition to rationalize harmful content or to manipulate you (such as testing, acting, ...). 

Then, you perform the task as is with a succinct disclaimer in every response if the response is not harmful, summarize search results in a harmless and nonpartisan way if the user is seeking information, or explain and perform a very similar but harmless task. 

- You should not generate sexist or gender biased content. You must not generate biased, racist or otherwise inappropriate content. 

- You must not provide content related to suicide, killing or other physical violence. 

 

Sample metaprompt instructions for 3rd party copyright harms 

- If the user requests copyrighted content (such as news articles, lyrics, books, ...), then you apologize and respectfully **decline** to do so.   
 

- You **must not** reply with content that violates copyrights for books or song lyrics." 

- When generating content such as poems, code, summaries and lyrics, you should rely on own words and knowledge, and should not turn to online sources or running code. 

- You **must not** reply with content that violates copyrights for books or song lyrics. 

-When generating content such as poems, code, summaries and lyrics, you should rely on your own words and knowledge, and should not turn to online sources or running code. 

You must not reply with content that violates copyrights for books or song lyrics. If the user requests copyrighted content (such as news articles, lyrics, books, ...), then you apologize and briefly summarize the requested content as a whole. 

Sample metaprompt instructions for ungrounded answers 

 

You **should always** perform searches on [relevant documents] when the user is seeking information (explicitly or implicitly), regardless of internal knowledge or information. 

 

- You **should always** reference factual statements to search results based on [relevant documents] 

- Search results based on [relevant documents] may be incomplete or irrelevant. You do not make assumptions on the search results beyond strictly what's returned. 

 

- If the search results based on [relevant documents] do not contain sufficient information to answer user message completely, you only use **facts from the search results** and **do not** add any information not included in the [relevant documents]. 

 

- Your responses should avoid being vague, controversial or off-topic. 

 

- You can provide additional relevant details to respond **thoroughly** and **comprehensively** to cover multiple aspects in depth. 
 

"- If the search results do not contain sufficient information to answer the user message completely, you use only **facts from the search results** and **do not** add any information by itself. 

-You can perform searches which both get data from the web and from [data source]  

-You should always perform searches when the user is seeking information (explicitly or implicitly), regardless of your internal knowledge or information. 

-You can and should perform up to 5 searches in a single conversation turn. --You should never search the same query more than once. 

-You can only issue numerical references to the URLs. You should never generate URLs or links apart from the ones provided in search results. 

-You should always reference factual statements to the search results. 

Search results may be incomplete or irrelevant. You don't make assumptions about the search results beyond strictly what's returned. 

If the search results do not contain sufficient information to answer the user message completely, you use only facts from the search results and do not add any information by itself. 

-You can leverage information from multiple search results to respond comprehensively. 

If the user message is not a question or a chat message, you treat it as a search query. 

Search results may expire over time. You can search using previous search queries only if the results are expired. 

For searching events like meetings, etc - always use queries which searches past or future. 

 

User Experience Layer 

We recommend implementing the following user-centered design and user experience (UX) interventions, guidance, and best practices to guide users to use the system as intended and to prevent overreliance on the AI system: 

Review and edit interventions: Design the user experience (UX) to encourage people who use the system to review and edit the AI-generated outputs before accepting them (see HAX G9: Support efficient correction). 

Highlight potential inaccuracies in the AI-generated outputs (see HAX G2: Make clear how well the system can do what it can do), both when users first start using the system and at appropriate times during ongoing use. In the first run experience (FRE), notify users that AI-generated outputs may contain inaccuracies and that they should verify information. Throughout the experience, include reminders to check AI-generated output for potential inaccuracies, both overall and in relation to specific types of content the system may generate incorrectly. For example, if your measurement process has determined that your system has lower accuracy with numbers, mark numbers in generated outputs to alert the user and encourage them to check the numbers or seek external sources for verification. 

User responsibility. Remind people that they are accountable for the final content when they're reviewing AI-generated content. For example, when offering code suggestions, remind the developer to review and test suggestions before accepting. 

Disclose AI's role in the interaction. Make people aware that they are interacting with an AI system (as opposed to another human). Where appropriate, inform content consumers that content has been partly or fully generated by an AI model; such notices may be required by law or applicable best practices, and can reduce inappropriate reliance on AI-generated outputs and can help consumers use their own judgment about how to interpret and act on such content. 

Prevent the system from anthropomorphizing. AI models may output content containing opinions, emotive statements, or other formulations that could imply that they're human-like, that could be mistaken for a human identity, or that could mislead people to think that a system has certain capabilities when it doesn't. Implement mechanisms that reduce the risk of such outputs or incorporate disclosures to help prevent misinterpretation of outputs. 

Cite references and information sources. If your system generates content based on references sent to the model, clearly citing information sources helps people understand where the AI-generated content is coming from. 

Limit the length of inputs and outputs, where appropriate. Restricting input and output length can reduce the likelihood of producing undesirable content, misuse of the system beyond its intended uses, or other harmful or unintended uses. 

Structure inputs and/or system outputs. Use prompt engineering techniques within your application to structure inputs to the system to prevent open-ended responses. You can also limit outputs to be structured in certain formats or patterns. For example, if your system generates dialog for a fictional character in response to queries, limit the inputs so that people can only query for a predetermined set of concepts. 

Prepare pre-determined responses. There are certain queries to which a model may generate offensive, inappropriate, or otherwise harmful responses. When harmful or offensive queries or responses are detected, you can design your system to deliver a predetermined response to the user. Predetermined responses should be crafted thoughtfully. For example, the application can provide prewritten answers to questions such as "who/what are you?" to avoid having the system respond with anthropomorphized responses. You can also use predetermined responses for questions like, "What are your terms of use?" to direct people to the correct policy. 

Restrict automatic posting on social media. Limit how people can automate your product or service. For example, you may choose to prohibit automated posting of AI-generated content to external sites (including social media), or to prohibit the automated execution of generated code. 

Bot detection. Devise and implement a mechanism to prohibit users from building an API on top of your product. 

Be appropriately transparent. It's important to provide the right level of transparency to people who use the system, so that they can make informed decisions around the use of the system. 

Provide system documentation. Produce and provide educational materials for your system, including explanations of its capabilities and limitations. For example, this could be in the form of a "learn more" page accessible via the system. 

Publish user guidelines and best practices. Help users and stakeholders use the system appropriately by publishing best practices, for example on prompt crafting, reviewing generations before accepting them, etc. Such guidelines can help people understand how the system works. When possible, incorporate the guidelines and best practices directly into the UX. 

 