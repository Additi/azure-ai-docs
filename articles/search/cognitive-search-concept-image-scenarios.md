---
title: Images in Cognitive Search | Microsoft Docs
description: Dealing with Images in Cognitive Search
services: search
manager: pablocas
author: luiscabrer
documentationcenter: ''

ms.assetid: 
ms.service: search
ms.devlang: NA
ms.workload: search
ms.topic: article
ms.tgt_pltfrm: na
ms.date: 05/01/2018
ms.author: heidist
---
# Dealing with Images for Cognitive Search Scenarios

Cognitive Search has several capabilities that allow you to work with images and image files.

## Getting Normalized Images

First of all, as part of the document cracking step, there is a new set of indexer configuration parameters that allow you to specify what an indexer should should do when it encounters image files or images embedded in files -- for instance a .PDF that contains several images inside it.

| Configuration Parameter | Description |
|--------------------|-------------|
| imageAction	| Set to "none" if no action should be taken when embedded images or image files are encountered. <br/>Set to either "generateNormalizedImages" to generate an array of normalized images as part of document cracking. These images will be exposed in the *normalized_images* field. <br/>The default is "none". This configuration is only pertinent to blob data sources, when "dataToExtract" is set to "contentAndMetadata". |
|  normalizedImageMaxWidth | The maximum width (in pixels) for normalized images generated. The default is 2000.|
|  normalizedImageMaxHeight | The maximum height (in pixels) for normalized images generated. The default is 2000.|


Note that the default of 2000 for the normalized images width and height is based on the maximum sizes supported by the [OCR skill](cognitive-search-skill-ocr.md) and the [image analysis skill](cognitive-search-skill-image-analysis.md). 


This is how you specify the imageAction in your [indexer definition](ref-create-indexer.md):

```json
{
  //...rest of your indexer definition goes here ...
  "parameters":
  {
    "configuration": 
    {
    	"dataToExtract": "contentAndMetadata",
     	"imageAction": "generateNormalizedImages"
    }
  }
}
```

When the *imageAction* is set to "generateNormalizedImages", the new *normalized_images* field will contain an array of Images, where each image is a complex type that has the following members.

| Image member       | Description                             |
|--------------------|-----------------------------------------|
| data               | BASE64 enconded string of a the normalized image in JPEG format.   |
| width              | Width of the normalized image in pixels. |
| height             | Height of the normalized image in pixels. |
| originalWidth      | The original width of the image before normalization. |
| originalHeight      | The original height of the image before normalization. |
| rotationFromOriginal |  Counter-clockwise rotation in degrees that occurred to create the normalized image. This is a value between 0 degrees and 360 degres. This step reads the metadata from the image that is generated by a camera or scanner. Usually a multiple of 90 degrees. |
| contentOffset | If the image was extracted from a document such as a .pdf or a .docx document, the charactor offset within the content field where the image was extracted from. |

 Sample value of *normalized_images*:
```json
[
  {
    "data": "BASE64 ENCODED STRING OF A JPEG IMAGE",
    "width": 500,
    "height": 300,
    "originalWidth": 5000,  
    "originalHeight": 3000,
    "rotationFromOriginal": 90,
    "contentOffset": 500  
  }
]
```

## Skills to deal with images

Currently we have two skills that take images as an input, the OCR skill and the analyze image skill. 

Note that at this point the analyze image skill and the OCR skill only work with images generated from the document cracking step. So the only supported input is "\document\normalized_images".

## Analyze Image Skill

The image analysis skill extracts a rich set of visual features based on the image content. For instance, you can generate a caption from an image, generate tags, or identify celebrities and landmarks.

Learn more about it [here](cognitive-search-skill-image-analysis.md).

## OCR Skill

This skill extracts text from image files such as JPGs, PNGs and bitmaps. It can extract text as well as the layout information of the text by providing bounding boxes for each of the strings identified.

The OCR skill allows you to select the algorithm to use for detecting text in your images. Currently it supports two algorithms, one for printed text and another on for handwritten text.

Learn more about it [here](cognitive-search-skill-ocr.md).


## Common Scenario: Dealing with files that contain embedded images

If you need to create a sing string text that includes all the content of the file (including all the text shown inside images embedded in the file), you need to perform the following steps:

1. Extract normalized_images. (as described earlier in this document)
1. OCR those images
1. Merge the text representation of those images with the text content extracted. You can use the [Text Merge](cognitive-search-skill-textmerger.md) skill to do that.

The following example skillset creates a *merged_text* field to contain the textual content of your document, as well as the OCRed text from each of the images embedded in that document. 

#### Request Body Syntax
```json
{
  "description": "Extract text from images and merge with content text to produce merged_text",
  "skills":
  [
    {
        "name": "OCR skill",
        "description": "Extract text (plain and structured) from image.",
        "@odata.type": "#Microsoft.Skills.Vision.OcrSkill",
        "context": "/document/normalized_images/*",
        "defaultLanguageCode": "en",
        "detectOrientation": true,
        "inputs": [
          {
            "name": "image",
            "source": "/document/normalized_images/*"
          }
        ],
        "outputs": [
          {
            "name": "text"
          }
        ]
    },
    {
      "@odata.type": "#Microsoft.Skills.Text.MergeSkill",
      "description": "Create merged_text, which includes all the textual representation of each image inserted at the right location in the content field.",
      "context": "/document",
      "insertPreTag": " ",
      "insertPostTag": " "
      "inputs": [
        {
          "name":"text", "source": "/document/content"
        },
        {
          "name": "itemsToInsert", "source": "/document/normalized_images/*/text"
        },
        {
          "name":"offsets", "source": "/document/normalized_images/*/contentOffset" 
        }
      ],
      "outputs": [
        {
          "name": "mergedText", "targetname" : "merged_text"
        }
      ]
    }
  ]
}
```

Now that you have a merged_text field, you could map that as a searchable field in your indexer definition, and all of the content of your files (including the text of the images) will be searchable!

## Common Scenario: Visualizing bounding boxes of extracted text

Whether you are planning to add search to a website or application, in some cases you may need to visualize search results layout information. For instance, you may want to highlight where a piece of text is found in an image.

Here are a few things to keep in mind:
+ Since the OCR step is taking place on the normalized images, the layout coordinates will be in the normalized image space. If you display the normalized image that would not be a problem, but there will be situations where you may want to display the original image. In that case you need to convert each of coordinate points in the layout information to the original image coordinate system. 

As a helper, if you need to transform normalized coordinates to the original coordinate space, you could use the following algorithm:
```csharp
        /// <summary>
        ///  Converts a point in the normalized coordinate space to the original coordinate space.
        ///  This method assumes the rotation angles are multiples of 90 degrees.
        /// </summary>
        public static Point GetOriginalCoordinates(Point normalized,
                                    int originalWidth,
                                    int originalHeight,
                                    int width,
                                    int height,
                                    double rotationFromOriginal)
        {
            Point original = new Point();
            double angle = rotationFromOriginal % 360;

            if (angle == 0 )
            {
                original.X = normalized.X;
                original.Y = normalized.Y;
            } else if (angle == 90)
            {
                original.X = normalized.Y;
                original.Y = (width - normalized.X);
            } else if (angle == 180)
            {
                original.X = (width -  normalized.X);
                original.Y = (height - normalized.Y);
            } else if (angle == 270)
            {
                original.X = height - normalized.Y;
                original.Y = normalized.X;
            }

            double scalingFactor = (angle % 180 == 0) ? originalHeight / height : originalHeight / width;
            original.X = (int) (original.X * scalingFactor);
            original.Y = (int)(original.Y * scalingFactor);

            return original;
        }
```

## See also
+ [Create indexer (REST)](ref-create-indexer.md)
+ [Analyze image skill](cognitive-search-skill-image-analysis.md)
+ [OCR skill](cognitive-search-skill-ocr.md)
+ [Text merge skill](cognitive-search-skill-textmerger.md)
+ [How to define a skillset](cognitive-search-defining-skillset.md)
+ [How to map enriched fields](cognitive-search-output-field-mapping.md)
