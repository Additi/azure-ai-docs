# Data, privacy, and security for use of models through the Model Catalog 

This article provides details regarding how data provided by you is processed, used, and stored when you deploy models from the Model Catalog. Additionally, see the [Microsoft Products and Services Data Protection Addendum](https://www.microsoft.com/licensing/docs/view/Microsoft-Products-and-Services-Data-Protection-Addendum-DPA), which governs data processing by Azure services. 

## What data is processed for models deployed in Azure Machine Learning? 

When you deploy models in Azure Machine Learning, the following types of data are processed to provide the service: 

* **Prompts and generated content.** Prompts are submitted by the user, and content (output) is generated by the model via the operations supported by the model. Prompts may include content that has been added via retrieval-augmented-generation (RAG), metaprompts, or other functionality included in an application.  
* **Training & validation data.** For models that support fine-tuning, you can provide your own training data, such as prompt-completion pairs, for the purposes of fine-tuning the model. 


## Generating inferencing outputs with pay-as-you-go deployments (aka Models-as-a-Service)  
When you deploy a model from the Model Catalog (base or fine-tuned, where available) to a pay-as-you-go managed endpoint for inferencing, an API is provisioned giving you access to the model in a central GPU managed by Microsoft. The model processes your input prompts and generates outputs based on the functionality of the model, as described in the model details provided for the model. While the model is provided by the model provider, and your use of the model is subject to the license terms provided with the model, Microsoft provides and manages the hosting infrastructure [and API endpoint]. The models are hosted in Azure Machine Learning infrastructure subject to Azure’s data, privacy, and security commitments. Learn more about [Azure compliance offerings](https://servicetrust.microsoft.com/DocumentPage/7adf2d9e-d7b5-4e71-bad8-713e6a183cf3). and the Azure infrastructure does not interact with any services operated by the model provider. [Microsoft policies applicable to models available for pay-as-you-go deployment prohibit those models from including any outbound network connections.]  

Microsoft acts as the data processor for prompts and outputs sent to and generated by a model deployed for pay-as-you-go inferencing. Microsoft does not share these prompts and outputs with the model provider, and Microsoft does not use these prompts and outputs to train or improve Microsoft’s, the model provider’s, or any third party’s models. Unless otherwise provided in the model details, models are stateless, and no prompts or outputs are stored in the model. Prompts and outputs are processed within the geography specified during deployment, but may be processed between regions within the geography for operational purposes (including performance and capacity management). 

## Generating inferencing outputs with real-time endpoints 

When you deploy a model from the Model Catalog to a real-time endpoint for inferencing, the model weights are deployed to dedicated Virtual Machines in your Azure Machine Learning workspace with managed online endpoints that make available a REST API for inference. You manage the infrastructure, and Azure’s data, privacy, and security commitments apply. [placeholder for statement re: no use of prompts/outputs for training.]  

Although containers for models “Curated by Azure AI” have been scanned for vulnerabilities that could exfiltrate data, not all models available through the Model Catalog have been scanned. To reduce the risk of data exfiltration, you can protect your deployment using virtual networks. Learn more: about [using virtual networks in Azure](./how-to-network-isolation-model-catalog). You can also use [Azure Policy to regulate the models](how-to-regulate-registry-deployments.md) that can be deployed by your users. 

## Fine-tuning a model for pay-as-you-go deployment 

If a model available for pay-as-you-go deployment supports fine-tuning, you can upload data to (or designate data already in) an [Azure Machine Learning Datastore](concept-data.md) to fine-tune the model. You can then create a pay-as-you-go deployment for the fine-tuned model. The fine-tuned model cannot be downloaded, but the fine-tuned model: 

* Is available exclusively for your use; 
* Can be [double encrypted at rest](../ai-services/openai/encrypt-data-at-rest.md) (by default with Microsoft's AES-256 encryption and optionally with a customer managed key). 
* Can be deleted by you at any time. 

Training data uploaded for fine-tuning is not used to train, retrain, or improve any Microsoft or third party model except as directed by you within the service.  

## Data processing for downloaded models 

If you download a model from the Model Catalog, you choose where to deploy the model, and you are responsible for how data is processed when you use the model.
