---
title: Monitor quality and token usage of deployed prompt flow applications (preview)
titleSuffix: Azure AI Studio
description: Learn how to monitor quality and token usage of deployed prompt flow applications with Azure AI Studio.
manager: scottpolly
ms.service: azure-ai-studio
ms.custom:
  - ignite-2023
ms.topic: how-to
ms.date: 04/24/2024
ms.reviewer: alehughes
reviewer: ahughes-msft
ms.author: mopeakande
author: msakande

---

# Monitor quality and token usage of deployed prompt flow applications (preview)

## Add preview disclaimer for AI Studio

Monitoring applications that are deployed to production is an essential part of the generative AI application lifecycle. Changes in data and consumer behavior can influence your application over time, resulting in outdated systems that negatively affect business outcomes and expose organizations to compliance, economic, and reputation risks. 

Azure AI monitoring for generative AI applications enables you to monitor your applications in production for token usage, generation quality, and operational metrics.

Capabilities and integrations for monitoring a prompt flow deployment include: 
- Collect production inference data from your deployed prompt flow application.
- Apply Responsible AI evaluation metrics such as groundedness, coherence, fluency, and relevance, which are interoperable with prompt flow evaluation metrics.
- Monitor prompt, completion, and total token usage across each model deployment in your prompt flow.
- Monitor operational metrics, such as request count, latency, and error rate.
- Preconfigured alerts and defaults to run monitoring on a recurring basis.
- Consume data visualizations and configure advanced behavior in Azure AI Studio.

## Prerequisites

# [Python SDK](#tab/python)

[!INCLUDE [basic prereqs sdk](includes/machine-learning-sdk-v2-prereqs.md)]

# [Studio](#tab/azure-studio)

Before following the steps in this article, make sure you have the following prerequisites:

* An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://azure.microsoft.com/free/).

* An Azure AI Studio hub and project. If you don't have these resources, use the steps in the [Azure AI Studio hubs for projects](./ai-resources.md) and [Create a project in Azure AI Studio](./create-projects.md) articles to create them.

## Monitoring metrics and requirements 

Monitoring metrics are generated by the following state-of-the-art GPT language models configured with specific evaluation instructions (prompt templates) which act as evaluator models for sequence-to-sequence tasks. This technique has strong empirical results and high correlation with human judgment when compared to standard generative AI evaluation metrics. For more information about prompt flow evaluation, see [submit bulk test and evaluate a flow](./flow-bulk-test-evaluation.md) and [evaluation and monitoring metrics for generative AI](../concepts/evaluation-metrics-built-in.md).

These GPT models are supported with monitoring and configured as your Azure OpenAI resource: 

- GPT-3.5 Turbo 
- GPT-4 
- GPT-4-32k 

The following metrics are supported for monitoring:

| Metric       | Description |
|--------------|-------------|
| Groundedness | Measures how well the model's generated answers align with information from the source data (user-defined context.) |
| Relevance    | Measures the extent to which the model's generated responses are pertinent and directly related to the given questions. |
| Coherence    | Measures the extent to which the model's generated responses are logically consistent and connected. |
| Fluency      | Measures the grammatical proficiency of a generative AI's predicted answer. |

When creating your flow, you need to ensure your column names are mapped. The following input data column names are used to measure generation safety and quality: 

| Input column name | Definition | Required |
|------|------------|----------|
| Prompt text | The original prompt given (also known as "inputs" or "question") | Required |
| Completion text | The final completion from the API call that is returned (also known as "outputs" or "answer") | Required |
| Context text | Any context data that is sent to the API call, together with original prompt. For example, if you hope to get search results only from certain certified information sources/website, you can define in the evaluation steps. | Optional |

What parameters are configured in your data asset dictates what metrics you can produce, according to this table: 

| Metric       | Prompt  | Completion | Context |
|--------------|---------|------------|---------|
| Coherence    | Required | Required   | -       | 
| Fluency      | Required | Required   | -       | 
| Groundedness | Required | Required   | Required|
| Relevance    | Required | Required   | Required|

For more information, see [question answering metric requirements](evaluate-generative-ai-app.md#question-answering-metric-requirements).

## Set up monitoring for prompt flow 

Follow these steps to set up monitoring for your prompt flow application: 

### 1. Deploy your prompt flow application with inferencing data collection

In this section, you will learn how to deploy your prompt flow with inferencing data collection enabled. For detailed information on deployment settings, see [Deploy a flow for real-time inference](https://review.learn.microsoft.com/en-us/azure/ai-studio/how-to/flow-deploy?branch=pr-en-us-273312). 

1. After creating your prompt flow, confirm it runs successfully and that the required inputs and outputs are configured for the [metrics you want to assess](#evaluation-metrics). The minimum required parameters of collecting only inputs and outputs provide only two metrics: coherence and fluency. You must configure your flow according to the [flow and metric configuration requirements](#flow-and-metric-configuration-requirements). In this example, we have `question` and `chat_history` as our inputs, and `answer` as our output.

    :::image type="content" source="../media/deploy-monitor/monitor/user-experience.png" alt-text="Screenshot of prompt flow editor with deploy button." lightbox = "../media/deploy-monitor/monitor/user-experience.png":::

1. Deploy your flow. Ensure that **Inferencing data collection** is enabled, which will seamlessly collect your application's inference data to Blob storage. This is required for monitoring. 

    :::image type="content" source="../media/deploy-monitor/monitor/basic-settings.png" alt-text="Screenshot of basic settings in the deployment wizard." lightbox = "../media/deploy-monitor/monitor/basic-settings.png":::

   After completing the **Advanced settings**, review the deployment configuration and deploy your flow by selecting **Create**.

    :::image type="content" source="../media/deploy-monitor/monitor/deployment-with-data-collection-enabled.png" alt-text="Screenshot of review page in the deployment wizard with all settins completed" lightbox = "../media/deploy-monitor/monitor/basic-settings-with-connections-specified.png":::

1. By default, all outputs of your deployment are collected using Azure AI's Model Data Collector. As an optional step, you can enter the advanced settings to confirm that your desired columns (for example, context of ground truth) are included in the endpoint response. 

    Your deployed flow needs to be configured in the following way:  
    - Flow inputs & outputs: You need to name your flow outputs appropriately and remember these column names when creating your monitor. In this article, we use the following settings: 
      - Inputs (required): "prompt" 
      - Outputs (required): "completion" 
      - Outputs (optional): "context" and/or "ground truth" 

    - Data collection: The **inferencing data collection** toggle must be enabled using Model Data Collector 

    - Outputs: In the prompt flow deployment wizard, confirm the required outputs are selected (such as completion, context, and ground_truth) that meet your metric configuration requirements.

1. Test your deployment in the deployment **Test** tab to ensure that it is working properly. 

    :::image type="content" source="../media/deploy-monitor/monitor/test-deploy.png" alt-text="Screenshot of the deployment test page." lightbox = "../media/deploy-monitor/monitor/test-deploy.png":::

    > [!NOTE]
    > Monitoring requires that at least one data point comes from a source other than the **Test** tab in the deployment. We recommend using the REST API available in the **Consume** tab to send sample requests to your deployment. More information on how to do so can be found [here](https://review.learn.microsoft.com/en-us/azure/ai-studio/how-to/flow-deploy?branch=pr-en-us-273312#create-an-online-deployment).

### 2. Configure monitoring

In this section, you will learn how to configure monitoring for your deployed prompt flow application. 

# [Studio](#tab/azure-studio)

1. Navigate to the **Deployments** tab and select the prompt flow deployment you just created after it has successfully been deployed. Select **Enable** within the **Generation quality montitoring** box. 

    :::image type="content" source="../media/deploy-monitor/monitor/deployment-page-highlight-monitoring.png" alt-text="Screenshot of the deployment page highlighting generation quality monitoring." lightbox = "../media/deploy-monitor/monitor/deployment-page-highlight-monitoring.png":::

1. Ensure your column names are mapped from your flow as defined in the previous requirements. 

    :::image type="content" source="../media/deploy-monitor/monitor/column-map.png" alt-text="Screenshot of columns mapped for monitoring metrics." lightbox = "../media/deploy-monitor/monitor/column-map.png":::

1. Select **Advanced settings** to adjust the sampling rate, thresholds for the configured metrics, and email addresses which should receive email alerts.

   :::image type="content" source="../media/deploy-monitor/monitor/column-map-advanced-options.png" alt-text="Screenshot of advanced options when mapping columns for monitoring metrics." lightbox = "../media/deploy-monitor/monitor/column-map-advanced-options.png":::

# [Python SDK](#tab/python)

You can use the following code to set up monitoring for your deployed prompt flow application:

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    MonitorSchedule,
    CronTrigger,
    MonitorDefinition,
    ServerlessSparkCompute,
    MonitoringTarget,
    AlertNotification,
    GenerationTokenStatisticsMonitorMetricThreshold,
    GenerationTokenStatisticsSignal,
    GenerationSafetyQualityMonitoringMetricThreshold,
    GenerationSafetyQualitySignal,
    BaselineDataRange,
    LlmData,
)
from azure.ai.ml.entities._inputs_outputs import Input

from azure.ai.ml.constants import MonitorTargetTasks, MonitorDatasetContext

# Authentication package
from azure.identity import DefaultAzureCredential

credential = DefaultAzureCredential()

# [START] update your azure resources details
subscription_id = "INSERT YOUR SUBSCRIPTION ID"
resource_group = "INSERT YOUR RESOURCE GROUP NAME"
workspace_name = "INSERT YOUR WORKSPACE NAME"
endpoint_name = "INSERT YOUR ENDPOINT NAME"
deployment_name = "INSERT YOUR DEPLOYMENT NAME"
aoai_deployment_name ="INSERT YOUR AOAI DEPLOYMENT NAME"
aoai_connection_name = "INSERT YOUR AOAI CONNECTION NAME"
app_trace_name = "app_traces"
app_trace_Version = "1"
monitor_name ="gen_ai_monitor_both_signals"
defaulttokenstatisticssignalname ="token-usage-signal"
defaultgsqsignalname ="gsq-signal"
trigger_schedule = CronTrigger(expression="15 10 * * *")
notification_emails_list = ["test@example.com", "def@example.com"]
#[End]

ml_client = MLClient(
    credential=credential,
    subscription_id=subscription_id,
    resource_group_name=resource_group,
    workspace_name=workspace_name,
)

spark_compute = ServerlessSparkCompute(instance_type="standard_e4s_v3", runtime_version="3.3")
monitoring_target = MonitoringTarget(
    ml_task=MonitorTargetTasks.QUESTION_ANSWERING,
    endpoint_deployment_id=f"azureml:{endpoint_name}:{deployment_name}",
)

# Create an instance of token statistic signal
def get_token_statistic_signal(token_count_threshold, token_count_per_group_threshold) -> GenerationTokenStatisticsSignal:
    totaltoken = {"total_token_count": token_count_threshold, "total_token_count_per_group": token_count_per_group_threshold}
    threshold = GenerationTokenStatisticsMonitorMetricThreshold(totaltoken = totaltoken)
    input_data = Input(
        type="uri_folder",
        path=f"{endpoint_name}-{deployment_name}-{app_trace_name}:{app_trace_Version}",
    )
    data_window = BaselineDataRange(lookback_window_size="P2D", lookback_window_offset="P0D")
    production_data = LlmData(
        data_column_names={"prompt_column": "question", "completion_column": "answer"},
        input_data=input_data,
        data_window=data_window,
    )
    
    return GenerationTokenStatisticsSignal(
        metric_thresholds = threshold,
        production_data = production_data)
    

# Create an instance of gsq signal
def get_gsq_signal(
        acceptable_fluency_score_per_instance : int,
        aggregated_fluency_pass_rate : float,
        acceptable_coherence_score_per_instance : int,
        aggregated_coherence_pass_rate : float) -> GenerationSafetyQualitySignal :
    generation_quality_thresholds = GenerationSafetyQualityMonitoringMetricThreshold(
        fluency={"acceptable_fluency_score_per_instance": acceptable_fluency_score_per_instance,
                 "aggregated_fluency_pass_rate": aggregated_fluency_pass_rate},
        coherence={"acceptable_coherence_score_per_instance": acceptable_coherence_score_per_instance,
                   "aggregated_coherence_pass_rate": aggregated_coherence_pass_rate},
    )
    input_data = Input(
        type="uri_folder",
        path=f"{endpoint_name}-{deployment_name}-{app_trace_name}:{app_trace_Version}",
    )
    data_window = BaselineDataRange(lookback_window_size="P7D", lookback_window_offset="P0D")
    production_data = LlmData(
        data_column_names={"prompt_column": "question", "completion_column": "answer"},
        input_data=input_data,
        data_window=data_window,
    )

    return GenerationSafetyQualitySignal(
        connection_id=f"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace_name}/connections/{aoai_connection_name}",
        metric_thresholds=generation_quality_thresholds,
        production_data=[production_data],
        sampling_rate=1.0,
        properties={
            "aoai_deployment_name": aoai_deployment_name,
            "enable_action_analyzer": "false",
            "azureml.modelmonitor.gsq_thresholds": '[{"metricName":"average_fluency","threshold":{"value":4}},{"metricName":"average_coherence","threshold":{"value":4}}]',
        },
    )

monitoring_signals = {
    defaultgsqsignalname: get_gsq_signal(4, 0.7, 4, 0.7),
    defaulttokenstatisticssignalname: get_token_statistic_signal(20,10),
    }

monitor_settings = MonitorDefinition(
compute=spark_compute,
monitoring_target=monitoring_target,
alert_notification=AlertNotification(emails=notification_emails_list),
)

model_monitor = MonitorSchedule(
    name = monitor_name,
    trigger=trigger_schedule,
    create_monitor=monitor_settings
)
ml_client.schedules.begin_create_or_update(model_monitor)
```

---

## Consume monitoring results 

After you have created your monitor, it will run daily to compute the token usage and generation quality metrics. Operational metrics are made available in near real-time from the **Operational** tab. Navigate to the **Monitoring (preview)** tab from within the deployment to view the monitoring results. The first thing you will see is an overview of monitoring results during the selected time window. You can use the date picker to change the time window of data you are monitoring. The following metrics are available in this overview:

- **Total request count**: The total number of requests which have been sent to the deployment during the selected time window.
- **Total token count**: The total number of tokens which have been used by the deployment during the selected time window.
- **Prompt token count**: The number of prompt tokens which have been used by the deployment during the selected time window.
- **Completion token count**: The number of completion tokens which have been used by the deployment during the selected time window.

Additionally, the **Token usage** tab is selected by default. In this tab, you can view the token usage of your application over time. You can also view the distribution of prompt and completion tokens over time. You can change the **Trendline scope** to monitor all tokens in the entire application or token usage for a particular deployment (e.g., gpt-4) used within your application. 

:::image type="content" source="../media/deploy-monitor/monitor/monitor-token-usage.png" alt-text="Screenshot showing the token usage on the deployment's monitoring page." lightbox = "../media/deploy-monitor/monitor/monitor-token-usage.png":::

Navigate to the **Generation quality** tab to monitor the quality of your application over time. The following metrics are shown in the timechart:

- **Violation count**: The violation count for a given metric (e.g., Fluency) is the sum of violations over the selected time window. A **violation** occurs for a metric when the metrics are computed (default is daily) if the computed value for the metric (e.g., 3.5) falls below the threshold (e.g., 4). 
- **Average score**: The average score for a given metric (e.g., Fluency) is the sum of the scores for all instances/requests divided by the number of instances/requests over the selected time window.

The **Generation quality violations** card shows the **violation rate** over the selected time window. The **violation rate** is the 

:::image type="content" source="../media/deploy-monitor/monitor/generation-quality-trendline.png" alt-text="Screenshot showing the generation quality trendline on the deployment's monitoring page." lightbox = "../media/deploy-monitor/monitor/generation-quality-trendline.png":::

From this view, you can also view a comprehensive table of all sampled requests sent to the deployment during the selected time window. 

    > [!NOTE]
    > Monitoring sets the default sampling rate at 10%. This means that if 100 requests are sent to your deployment, 10 would be sampled and used to compute the generation quality metrics. You can adjust the sampling rate in the settings. 

:::image type="content" source="../media/deploy-monitor/monitor/generation-quality-tracing-information.png" alt-text="Screenshot showing the trace button for the generation quality." lightbox = "../media/deploy-monitor/monitor/generation-quality-tracing-information.png":::

If you are curious about the tracing details for a given request, select the **Trace** button on the right side of the row in the table. This view provides comprehensive trace details for the request to your application. 

:::image type="content" source="../media/deploy-monitor/monitor/trace-information.png" alt-text="Screenshot showing the trace information." lightbox = "../media/deploy-monitor/monitor/trace-information.png":::

You can also view the operational metrics for the deployment by navigating to the **Operational** tab. We support the following operational metrics:

- Request count
- Latency
- Error rate

:::image type="content" source="../media/deploy-monitor/monitor/deployment-operational-tab.png" alt-text="Screenshot of the operational tab for the deployment." lightbox = "../media/deploy-monitor/monitor/deployment-operational-tab.png":::

Based on the results in the **Monitoring (preview)** tab for your deployment, you are provided with insights to help you proactively improve the performance of your prompt flow application. 

## Advanced monitoring configuration with SDK v2

Monitoring also supports advanced configuration options with the SDK v2. The following scenarios are supported:

### 1. Enable monitoring for token usage 

If you are only interested in enabling token usage monitoring for your deployed prompt flow application, you can adapt the script below to your scenario: 

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    MonitorSchedule,
    CronTrigger,
    MonitorDefinition,
    ServerlessSparkCompute,
    MonitoringTarget,
    AlertNotification,
    GenerationTokenStatisticsMonitorMetricThreshold,
    GenerationTokenStatisticsSignal,
    BaselineDataRange,
    LlmData,
)
from azure.ai.ml.entities._inputs_outputs import Input

from azure.ai.ml.constants import MonitorTargetTasks, MonitorDatasetContext

# Authentication package
from azure.identity import DefaultAzureCredential

credential = DefaultAzureCredential()

# update your azure resources details
subscription_id = "INSERT YOUR SUBSCRIPTION ID"
resource_group = "INSERT YOUR RESOURCE GROUP NAME"
workspace_name = "INSERT YOUR PROJECT / WORKSPACE NAME"
endpoint_name = "INSERT YOUR ENDPOINT NAME"
deployment_name = "INSERT YOUR DEPLOYMENT NAME"
app_trace_name = "app_traces"
app_trace_Version = "1"
monitor_name ="gen_ai_monitor_out_of_box"
defaulttokenstatisticssignalname ="token-usage-signal"
trigger_schedule = CronTrigger(expression="15 10 * * *")
notification_emails_list = ["test@example.com", "def@example.com"]

ml_client = MLClient(
    credential=credential,
    subscription_id=subscription_id,
    resource_group_name=resource_group,
    workspace_name=workspace_name,
)

spark_compute = ServerlessSparkCompute(instance_type="standard_e4s_v3", runtime_version="3.3")
monitoring_target = MonitoringTarget(
    ml_task=MonitorTargetTasks.QUESTION_ANSWERING,
    endpoint_deployment_id=f"azureml:{endpoint_name}:{deployment_name}",
)

# Create an instance of token statistic signal
def get_token_statistic_signal(token_count_threshold, token_count_per_group_threshold) -> GenerationTokenStatisticsSignal:
    totaltoken = {"total_token_count": token_count_threshold, "total_token_count_per_group": token_count_per_group_threshold}
    threshold = GenerationTokenStatisticsMonitorMetricThreshold(totaltoken = totaltoken)
    input_data = Input(
        type="uri_folder",
        path=f"{endpoint_name}-{deployment_name}-{app_trace_name}:{app_trace_Version}",
    )
    data_window = BaselineDataRange(lookback_window_size="P2D", lookback_window_offset="P0D")
    production_data = LlmData(
        data_column_names={"prompt_column": "question", "completion_column": "answer"},
        input_data=input_data,
        data_window=data_window,
    )
    
    return GenerationTokenStatisticsSignal(
        metric_thresholds = threshold,
        production_data = production_data)

monitoring_signals = {
    defaulttokenstatisticssignalname: get_token_statistic_signal(20,10),
    }

monitor_settings = MonitorDefinition(
compute=spark_compute,
monitoring_target=monitoring_target,
alert_notification=AlertNotification(emails=notification_emails_list),
)

model_monitor = MonitorSchedule(
    name = monitor_name,
    trigger=trigger_schedule,
    create_monitor=monitor_settings
)

ml_client.schedules.begin_create_or_update(model_monitor)
```

### 2. Enable monitoring for generation quality 

If you are only interested in enabling generation quality monitoring for your deployed prompt flow application, you can adapt the script below to your scenario: 

```python
from azure.ai.ml import MLClient
from azure.ai.ml.entities import (
    MonitorSchedule,
    CronTrigger,
    MonitorDefinition,
    ServerlessSparkCompute,
    MonitoringTarget,
    AlertNotification,
    GenerationSafetyQualityMonitoringMetricThreshold,
    GenerationSafetyQualitySignal,
    BaselineDataRange,
    LlmData,
)
from azure.ai.ml.entities._inputs_outputs import Input

from azure.ai.ml.constants import MonitorTargetTasks, MonitorDatasetContext

# Authentication package
from azure.identity import DefaultAzureCredential

credential = DefaultAzureCredential()

# update your azure resources details
subscription_id = "INSERT YOUR SUBSCRIPTION ID"
resource_group = "INSERT YOUR RESOURCE GROUP NAME"
workspace_name = "INSERT YOUR WORKSPACE NAME"
endpoint_name = "INSERT YOUR ENDPOINT NAME"
deployment_name = "INSERT YOUR DEPLOYMENT NAME"
aoai_deployment_name ="INSERT YOUR AOAI DEPLOYMENT NAME"
aoai_connection_name = "INSERT YOUR AOAI CONNECTION NAME"
app_trace_name = "app_traces"
app_trace_Version = "1"
monitor_name ="gen_ai_monitor_gsq_only_explicity"
defaultgsqsignalname ="gsq-signal"
trigger_schedule = CronTrigger(expression="15 10 * * *")
notification_emails_list = ["test@example.com", "def@example.com"]

ml_client = MLClient(
    credential=credential,
    subscription_id=subscription_id,
    resource_group_name=resource_group,
    workspace_name=workspace_name,
)

spark_compute = ServerlessSparkCompute(instance_type="standard_e4s_v3", runtime_version="3.3")
monitoring_target = MonitoringTarget(
    ml_task=MonitorTargetTasks.QUESTION_ANSWERING,
    endpoint_deployment_id=f"azureml:{endpoint_name}:{deployment_name}",
)

# Create an instance of gsq signal
def get_gsq_signal(
        acceptable_fluency_score_per_instance : int,
        aggregated_fluency_pass_rate : float,
        acceptable_coherence_score_per_instance : int,
        aggregated_coherence_pass_rate : float) -> GenerationSafetyQualitySignal :
    generation_quality_thresholds = GenerationSafetyQualityMonitoringMetricThreshold(
        fluency={"acceptable_fluency_score_per_instance": acceptable_fluency_score_per_instance,
                 "aggregated_fluency_pass_rate": aggregated_fluency_pass_rate},
        coherence={"acceptable_coherence_score_per_instance": acceptable_coherence_score_per_instance,
                   "aggregated_coherence_pass_rate": aggregated_coherence_pass_rate},
    )
    input_data = Input(
        type="uri_folder",
        path=f"{endpoint_name}-{deployment_name}-{app_trace_name}:{app_trace_Version}",
    )
    data_window = BaselineDataRange(lookback_window_size="P7D", lookback_window_offset="P0D")
    production_data = LlmData(
        data_column_names={"prompt_column": "question", "completion_column": "answer"},
        input_data=input_data,
        data_window=data_window,
    )

    return GenerationSafetyQualitySignal(
        connection_id=f"/subscriptions/{subscription_id}/resourceGroups/{resource_group}/providers/Microsoft.MachineLearningServices/workspaces/{workspace_name}/connections/{aoai_connection_name}",
        metric_thresholds=generation_quality_thresholds,
        production_data=[production_data],
        sampling_rate=1.0,
        properties={
            "aoai_deployment_name": aoai_deployment_name,
            "enable_action_analyzer": "false",
            "azureml.modelmonitor.gsq_thresholds": '[{"metricName":"average_fluency","threshold":{"value":4}},{"metricName":"average_coherence","threshold":{"value":4}}]',
        },
    )


monitoring_signals = {
    defaultgsqsignalname: get_gsq_signal(4, 0.7, 4, 0.7),
    }

monitor_settings = MonitorDefinition(
compute=spark_compute,
monitoring_target=monitoring_target,
alert_notification=AlertNotification(emails=notification_emails_list),
)

model_monitor = MonitorSchedule(
    name = monitor_name,
    trigger=trigger_schedule,
    create_monitor=monitor_settings
)
ml_client.schedules.begin_create_or_update(model_monitor)
```

After you have created your monitor from the SDK, you can [consume the monitoring](#consume-monitoring-results) results in AI Studio. 

## Next steps

- Learn more about what you can do in [Azure AI Studio](../what-is-ai-studio.md)
- Get answers to frequently asked questions in the [Azure AI FAQ article](../faq.yml)
