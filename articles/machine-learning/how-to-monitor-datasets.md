---
title: Analyze and monitor for data drift on datasets (preview)
titleSuffix: Azure Machine Learning
description: Create Azure Machine Learning datasets monitors (preview), monitor for data drift in datasets, and set up alerts.
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: how-to
ms.reviewer: sgilley
ms.author: copeters
author: lostmygithubaccount
ms.date: 06/15/2020

## Customer intent: As a data scientist, I want to create dataset monitors and set alerts to monitor data drift in my datasets.
---

# Detect data drift (preview) on datasets
[!INCLUDE [applies-to-skus](../../includes/aml-applies-to-basic-enterprise-sku.md)]

Learn how to create a dataset and dataset monitors and set alerts to monitor data drift.

With Azure Machine Learning dataset monitors (preview), you can:
* **Analyze drift in your data** to understand how it changes over time.
* **Monitor model data** for differences between training and serving datasets.
* **Monitor new data** for differences between any baseline and target dataset.
* **Profile features in data** to track how statistical properties change over time.
* **Set up alerts on data drift** for early warnings to potential issues. 

An [Azure Machine learning dataset](how-to-create-register-datasets.md) is used to create the monitor. The dataset must include a timestamp column.

Metrics and insights are available through the [Azure Application Insights](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview) resource associated with the Azure Machine Learning workspace.

> [!Important]
> Monitoring data drift with the SDK is available in all editions. However, monitoring data drift through the studio on the web is Enterprise edition only.

## Prerequisites

To create and work with dataset monitors, you need:
* An Azure subscription. If you don't have an Azure subscription, create a free account before you begin. Try the [free or paid version of Azure Machine Learning](https://aka.ms/AMLFree) today.
* An [Azure Machine Learning workspace](how-to-manage-workspace.md).
* The [Azure Machine Learning SDK for Python installed](https://docs.microsoft.com/python/api/overview/azure/ml/install?view=azure-ml-py), which includes the azureml-datasets package.
* Structured (tabular) data with a timestamp specified in the file path, file name, or column in the data.

## What is data drift?

Data drift is one of the top reasons model accuracy degrades over time.  For machine learning models, data drift is the change in model input data that leads to model performance degradation.  Monitoring data drift helps detect these model performance issues.

Causes of data drift include:

- Upstream process changes, such as a sensor being replaced that changes the units of measurement from inches to centimeters. 
- Data quality issues, such as a broken sensor always reading 0.
- Natural drift in the data, such as mean temperature changing with the seasons.
- Change in relation between features, or covariate shift. 

Azure Machine Learning simplifies drift detection by computing a single metric abstracting the complexity of datasets being compared.  These datasets may have hundreds of features and tens of thousands of rows. Once drift is detected, you drill down into which features are causing the drift.  You then inspect feature level metrics to debug and isolate the root cause for the drift.

This top down approach makes it easy to monitor data instead of traditional rules-based techniques. Rules-based techniques such as allowed data range or allowed unique values can be time consuming and error prone.

With Azure Machine Learning dataset monitors you can set up alerts that assist in data drift detection in datasets over time.

### Dataset monitors 

with a dataset monitor you can:

* Detect and alert to data drift on new data in a dataset.
* Analyze historical data for drift.
* Profile new data over time.

The data drift algorithm provides an overall measure of change in data and indication of which features are responsible for further investigation. Dataset monitors produce a number of other metrics by profiling new data in the `timeseries` dataset. 

Custom alerting can be set up on all metrics generated by the monitor through [Azure Application Insights](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview). Dataset monitors can be used to quickly catch data issues and reduce the time to debug the issue by identifying likely causes.  

Conceptually, there are three primary scenarios for setting up dataset monitors in Azure Machine Learning.

Scenario | Description
---|---
Monitor a model's serving data for drift from the training data | Results from this scenario can be interpreted as monitoring a proxy for the model's accuracy, since model accuracy degrades when the serving data drifts from the training data.
Monitor a time series dataset for drift from a previous time period. | This scenario is more general, and can be used to monitor datasets involved upstream or downstream of model building.  The target dataset must have a timestamp column. The baseline dataset can be any tabular dataset that has features in common with the target dataset.
Perform analysis on past data. | This scenario can be used to understand historical data and inform decisions in settings for dataset monitors.

Dataset monitors depend on the following Azure services.

|Azure service  |Description  |
|---------|---------|
| *Dataset* | Drift uses Machine Learning datasets to retrieve training data and compare data for model training.  Generating profile of data is used to generate some of the reported metrics such as min, max, distinct values, distinct values count. |
| *Azureml pipeline and compute* | The drift calculation job is hosted in azureml pipeline.  The job is triggered on demand or by schedule to run on a compute configured at drift monitor creation time.
| *Application insights*| Drift emits metrics to Application Insights belonging to the machine learning workspace.
| *Azure blob storage *| Drift emits metrics in json format to Azure blob storage.

## How dataset monitors data

Use Machine Learning datasets to monitor for data drift. Specify a baseline dataset - usually the training dataset for a model. A target dataset - usually model input data - is compared over time to your baseline dataset. This comparison means that your target dataset must have a timestamp column specified.

## Create target dataset

The target dataset needs the `timeseries` trait set on it by specifying the timestamp column either from a column in the data or a virtual column derived from the path pattern of the files. Create the dataset with a timestamp through the Python SDK or Azure Machine Learning studio. A column representing a "fine grain" timestamp must be specified to add `timeseries` trait to the dataset. If your data is partitioned into folder structure with time info, such as '{yyyy/MM/dd}', create a virtual column through the path pattern setting and set it as the "coarse grain" timestamp to improve the importance of time series functionality.

### Python SDK

The [`Dataset`](https://docs.microsoft.com/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py#with-timestamp-columns-timestamp-none--partition-timestamp-none--validate-false----kwargs-) class' [`with_timestamp_columns()`](https://docs.microsoft.com/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py#with-timestamp-columns-timestamp-none--partition-timestamp-none--validate-false----kwargs-)  method defines the time stamp column for the dataset.

```python 
from azureml.core import Workspace, Dataset, Datastore

# get workspace object
ws = Workspace.from_config()

# get datastore object 
dstore = Datastore.get(ws, 'your datastore name')

# specify datastore paths
dstore_paths = [(dstore, 'weather/*/*/*/*/data.parquet')]

# specify partition format
partition_format = 'weather/{state}/{date:yyyy/MM/dd}/data.parquet'

# create the Tabular dataset with 'state' and 'date' as virtual columns 
dset = Dataset.Tabular.from_parquet_files(path=dstore_paths, partition_format=partition_format)

# assign the timestamp attribute to a real or virtual column in the dataset
dset = dset.with_timestamp_columns('date')

# register the dataset as the target dataset
dset = dset.register(ws, 'target')
```

For a full example of using the `timeseries` trait of datasets, see the [example notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/datasets-tutorial/timeseries-datasets/tabular-timeseries-dataset-filtering.ipynb) or the [datasets SDK documentation](https://docs.microsoft.com/python/api/azureml-core/azureml.data.tabulardataset?view=azure-ml-py#with-timestamp-columns-timestamp-none--partition-timestamp-none--validate-false----kwargs-).

### Azure Machine Learning studio
[!INCLUDE [applies-to-skus](../../includes/aml-applies-to-enterprise-sku-inline.md)]

If you create your dataset using Azure Machine Learning studio, ensure the path to your data contains timestamp information, include all subfolders with data, and set the partition format. 

In the following example, all data under the subfolder *NoaaIsdFlorida/2019* is taken, and the partition format specifies the timestamp's year, month, and day.

[![Partition format](./media/how-to-monitor-datasets/partition-format.png)](media/how-to-monitor-datasets/partition-format-expand.png)

In the **Schema** settings, specify the timestamp column from a virtual or real column in the specified dataset:

![Timestamp](./media/how-to-monitor-datasets/timestamp.png)

## Create dataset monitors

Create dataset monitors to detect and alert to data drift on a new dataset.  Use either the Python SDK or Azure Machine Learning studio.

### Python SDK

See the [Python SDK reference documentation on data drift](/python/api/azureml-datadrift/azureml.datadrift) for full details. 

The following example shows how to create a dataset monitor using the Python SDK

```python
from azureml.core import Workspace, Dataset
from azureml.datadrift import DataDriftDetector
from datetime import datetime

# get the workspace object
ws = Workspace.from_config()

# get the target dataset
dset = Dataset.get_by_name(ws, 'target')

# set the baseline dataset
baseline = target.time_before(datetime(2019, 2, 1))

# set up feature list
features = ['latitude', 'longitude', 'elevation', 'windAngle', 'windSpeed', 'temperature', 'snowDepth', 'stationName', 'countryOrRegion']

# set up data drift detector
monitor = DataDriftDetector.create_from_datasets(ws, 'drift-monitor', baseline, target, 
                                                      compute_target='cpu-cluster', 
                                                      frequency='Week', 
                                                      feature_list=None, 
                                                      drift_threshold=.6, 
                                                      latency=24)

# get data drift detector by name
monitor = DataDriftDetector.get_by_name(ws, 'drift-monitor')

# update data drift detector
monitor = monitor.update(feature_list=features)

# run a backfill for January through May
backfill1 = monitor.backfill(datetime(2019, 1, 1), datetime(2019, 5, 1))

# run a backfill for May through today
backfill1 = monitor.backfill(datetime(2019, 5, 1), datetime.today())

# disable the pipeline schedule for the data drift detector
monitor = monitor.disable_schedule()

# enable the pipeline schedule for the data drift detector
monitor = monitor.enable_schedule()
```

For a full example of setting up a `timeseries` dataset and data drift detector, see our [example notebook](https://aka.ms/datadrift-notebook).

### Azure Machine Learning studio
[!INCLUDE [applies-to-skus](../../includes/aml-applies-to-enterprise-sku-inline.md)]

To set up alerts on your dataset monitor, the workspace that contains the dataset you want to create a monitor for must have Enterprise edition capabilities.

After the workspace functionality is confirmed, navigate to the [studio's homepage](https://ml.azure.com) and select the **Datasets** tab on the left. Select **Dataset monitors**.

![Monitor list](./media/how-to-monitor-datasets/monitor-list.png)

Click on the **+Create monitor** button and continue through the wizard by clicking **Next**.  

:::image type="content" source="media/how-to-monitor-datasets/wizard.png" alt-text="Create a monitor wizard":::

* **Select target dataset**.  The target dataset is a tabular dataset with timestamp column specified which will be analyzed for data drift. The target dataset must have features in common with the baseline dataset, and should be a `timeseries` dataset, which new data is appended to. Historical data in the target dataset can be analyzed, or new data can be monitored.

* **Select baseline dataset.**  Select the tabular dataset to be used as the baseline for comparison of the target dataset over time.  The baseline dataset must have features in common with the target dataset.  The baseline can be either a slice of the target dataset or a model's training dataset.

* **Monitor settings**.  These settings are for the scheduled dataset monitor pipeline, which will be created. 

    | Setting | Description | Tips | Mutable | 
    | ------- | ----------- | ---- | ------- |
    | Name | Name of the dataset monitor. | | No |
    | Features | List of features that will be analyzed for data drift over time. | Set to a model's output feature(s) to measure concept drift. Don't include features that naturally drift over time (month, year, index, etc.). You can backfill and existing data drift monitor after adjusting the list of features. | Yes | 
    | Compute target | Azure Machine Learning compute target to run the dataset monitor jobs. | | Yes | 
    | Enable | Enable or disable the schedule on the dataset monitor pipeline | Disable the schedule to analyze historical data with the backfill setting. It can be enabled after the dataset monitor is created. | Yes | 
    | Frequency | The frequency that will be used to schedule the pipeline job and analyze historical data if running a backfill. Options include daily, weekly, or monthly. | Adjust this setting to include a comparable size of data to the baseline. | No | 
    | Latency | Time, in hours, it takes for data to arrive in the dataset. For instance, if it takes three days for data to arrive in the SQL DB the dataset encapsulates, set the latency to 72. | Cannot be changed after the dataset monitor is created | No | 
    | Email addresses | Email addresses for alerting based on breach of the data drift percentage threshold. | Emails are sent through Azure Monitor. | Yes | 
    | Threshold | Data drift percentage threshold for email alerting. | Further alerts and events can be set on many other metrics in the workspace's associated Application Insights resource. | Yes |

After finishing the wizard, the resulting dataset monitor will appear in the list. Select it to go to that monitor's details page.

## Understand data drift results

The data monitor produces two groups of results: **Drift overview** and **Feature details**. The following animation shows the available drift monitor charts based on the selected feature and metric.

![Demo video](./media/how-to-monitor-datasets/video.gif)

### Drift overview

The **Drift overview** section contains top-level insights into the magnitude of data drift and highlights features to be further investigated. 

| Metric | Description | Tips | 
| ------ | ----------- | ---- | 
| Data drift magnitude | A percentage of drift between the baseline and target dataset over time. Ranging from 0 to 100, 0 indicates identical datasets and 100 indicates the Azure Machine Learning data drift model can completely tell the two datasets apart. | Noise in the precise percentage measured is expected due to machine learning techniques being used to generate this magnitude. | 
| Drift contribution by feature | Contribution of each feature in the target dataset to the measured drift magnitude. |  Due to covariate shift, the underlying distribution of a feature does not necessarily need to change to have relatively high feature importance. | 

The following image is an example of charts seen in the **Drift overview**  results in Azure Machine Learning studio, resulting from a backfill of [NOAA Integrated Surface Data](https://azure.microsoft.com/services/open-datasets/catalog/noaa-integrated-surface-data/). Data was sampled to `stationName contains 'FLORIDA'`, with January 2019 being used as the baseline dataset and all 2019 data used as the target.
 
![Drift overview](./media/how-to-monitor-datasets/drift-overview.png)

### Feature details

The **Feature details** section contains feature-level insights into the change in the selected feature's distribution, as well as other statistics, over time. 

The target dataset is also profiled over time. The statistical distance between the baseline distribution of each feature is compared with the target dataset's over time, which is conceptually similar to the data drift magnitude with the exception that this statistical distance is for an individual feature. Min, max, and mean are also available. 

In the Azure Machine Learning studio, if you click on a data point in the graph the distribution of the feature being shown will adjust accordingly. By default, it shows the baseline dataset's distribution and the most recent run's distribution of the same feature. 

These metrics can also be retrieved in the Python SDK through the `get_metrics()` method on a `DataDriftDetector` object.

#### Numeric features 

Numeric features are profiled in each dataset monitor run. The following are exposed in the Azure Machine Learning studio. Probability density is shown for the distribution.

| Metric | Description |  
| ------ | ----------- |  
| Wasserstein distance | Minimum amount of work to transform baseline distribution into the target distribution. |
| Mean value | Average value of the feature. |
| Min value | Minimum value of the feature. |
| Max value | Maximum value of the feature. |

The following metrics are available for data drift:

|Metric  |Description  |
|---------|---------|
| Data drift detection algorithm |  A binary classifier is trained on a concatenated dataset with labels. The performance of the classifier translates to the statistical dissimilarity of the input datasets.       |
|Magnitude     |   Based on Matthews correlation coefficient (MCC), a balanced metric assessing a binary classifier's performance taking into account all four components of confusion matrix. In the context of a binary classifier trained to detect drift, interpretation of the MCC metric translates well and has the following meaning: Considering MCC from 0 to 1, 0 indicates that the binary classifier was not able to classify two datasets apart from each other better than chance implying that the statistical properties of the two datasets are very similar i.e. no drift, where 1 indicates perfect prediction implying a substantial drift.      |
|Contribution by feature     |  Based on feature importance reported in the underlying binary classifier machine learning model.      |
|Euclidian distance     |  Computed for categorical columns. Euclidean distance is computed on two vectors, generated from empirical distribution of the same categorical column from two datasets. 0 indicates there is no difference in the empirical distributions.  The more it deviates from 0, the more this column has drifted. Trends can be observed from a time series plot of this metric and can be helpful in uncovering a drifting feature.  |

![Feature details numeric](./media/how-to-monitor-datasets/feature-details.png)

#### Categorical features 

Numeric features are profiled in each dataset monitor run. The following are exposed in the Azure Machine Learning studio. A histogram is shown for the distribution.

| Metric | Description |  
| ------ | ----------- |  
| Euclidian distance | Geometric distance between baseline and target distributions. |
| Unique values | Number of unique values (cardinality) of the feature. |


![Feature details categorical](./media/how-to-monitor-datasets/feature-details2.png)

## Metrics, alerts, and events

Metrics can be queried in the [Azure Application Insights](https://docs.microsoft.com/azure/azure-monitor/app/app-insights-overview) resource associated with your machine learning workspace. Which gives access to all features of Application Insights including set up for custom alert rules and action groups to trigger an action such as, an Email/SMS/Push/Voice or Azure Function. Please refer to the complete Application Insights documentation for details. 

To get started, navigate to the [Azure portal](https://portal.azure.com) and select your workspace's **Overview** page.  The associated Application Insights resource is on the far right:

[![Azure portal overview](./media/how-to-monitor-datasets/ap-overview.png)](media/how-to-monitor-datasets/ap-overview-expanded.png)

Select Logs (Analytics) under Monitoring on the left pane:

![Application insights overview](./media/how-to-monitor-datasets/ai-overview.png)

The dataset monitor metrics are stored as `customMetrics`. You can write and run a query after setting up a dataset monitor to view them:

[![Log analytics query](./media/how-to-monitor-datasets/simple-query.png)](media/how-to-monitor-datasets/simple-query-expanded.png)

After identifying metrics to set up alert rules, create a new alert rule:

![New alert rule](./media/how-to-monitor-datasets/alert-rule.png)

You can use an existing action group, or create a new one to define the action to be taken when the set conditions are met:

![New action group](./media/how-to-monitor-datasets/action-group.png)

## Next steps

* Head to the [Azure Machine Learning studio](https://ml.azure.com) or the [Python notebook](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/work-with-data/datadrift-tutorial/datadrift-tutorial.ipynb) to set up a dataset monitor.
* See how to set up data drift on [models deployed to Azure Kubernetes Service](how-to-monitor-data-drift.md).
* Set up dataset drift monitors with [event grid](how-to-use-event-grid.md). 
* See these common [troubleshooting tips](resource-known-issues.md#data-drift) if you're having problems.
