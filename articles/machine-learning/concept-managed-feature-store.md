---
title: Managed feature store in Azure Machine Learning
titleSuffix: Azure Machine Learning
description: Learn about how Azure Machine Learning uses managed feature stores to create data transformation features and make these features available for training and deployment.
author: rsethur
ms.author: seramasu
ms.reviewer: franksolomon
ms.service: machine-learning
ms.subservice: mldata 
ms.topic: conceptual
ms.date: 05/23/2023 
ms.custom: template-concept 
---


# Managed feature store in Azure Machine Learning

<!-- 2. Introductory paragraph 
Required. Lead with a light intro that describes what the article covers. Answer the 
fundamental “why would I want to know this?” question. Keep it short.
-->

[pending intro paragraph]


## Feature store concepts

:::image type="content" source="./media/concept-managed-feature-store/concepts.png" alt-text="A conceptual diagram describing the concepts and components of the managed feature store":::

You can create and manage feature sets through a feature store. Feature sets are a collection of features generated by applying transformation on a source system data. Feature sets encapsulate a source, the transformation function, and the materialization settings. Currently we support PySpark feature transformation code.

You start by creating a feature set specification. A Feature set specification is a self-contained definition of a feature set that can be developed and tested locally.

A feature set specification typically consists of the following components:
1. `source`: What source(s) does this feature map to
1. `transformation` (optional): The transformation logic that needs to be applied to the source data to create features. In our case, we use Spark as the supported compute.
1. The names of the columns represent the `index_columns` and the `timestamp_column`: This is required when users try to join feature data with observation data.
1. `materialization_settings`(optional): Required if you want to cache the feature values in an offline/online store for efficient retrieval.

Once you have developed and tested the feature set specification in your local/dev environment, you register it as a feature set asset in the feature store in order to get managed capabilities.


## Walkthrough with an example:

Let's walkthrough an example of how feature store benefits a marketing department. We need to create a churn model and we would like to include features on customer transactions.
 
### Step 1: Create a feature set spec

#### Step 1a: Identify source for the feature set
The `source` for this feature set is `customer_transactions_table` in the data lake. It includes the customer ID, purchase amount, and event timestamp.

| customer_id | timestamp               | spend  |
|---------|-------------------------|--------|
| 01      | 2022-07-31 23:11:00.000 | 25.53  |
| 01      | 2022-08-01 01:12:0.000  | 20.9   |
| 01      | 2022-08-01 02:22:0.000  | 100.01 |
| 01      | 2022-08-01 03:22:0.000  | 125.3  |
| 01      | 2022-08-01 12:01:0.000  | 25.5   |
| 01      | 2022-08-01 12:30:0.000  | 60.0   |
| 01      | 2022-08-01 23:11:59.000 | 55.5   |
| 02      | 2022-08-01 23:11:00.000 | 25.53  |
|...| ...|... |

#### Step 1b: Create features by defining transformation logic
We want windowed aggregates on customer transactions, for example `spend_6hr_sum` (money spent in a rolling 6-hr window), `spend_1day_sum`, `transactions_6hr_sum` (number of transactions in rolling 6-hr window) and `transactions_1day_sum`. We create transformation logic that can be applied to the source data to produce the above features. 

Our feature store Spark-based transformations. Sample code:
<!-- need clarity on above sentence. Not sure what it means.-->

```yaml
#source ../../../configs/featurestore/featurestore/feature_sets/customer_transactions/spec/code/foo/transformer.py
from typing import Dict
from pyspark.sql import Dataframe
from pyspark.sql.types import StructType, StructField, DoubleType, TimestampType
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.ml import Transformer

class CustomerTransactionsTransformer(Transformer):

    def _transform(df: Dataframe) -> Dataframe:
        hours = lambda i: i * 3600
        w_6h = (Window.partitionBy("customer_id").orderBy(F.col("ts").cast("timestamp").cast('long')).rangeBetween(-hours(6), 0))
        days = lambda i: i * 86400
        w_1d = (Window.partitionBy("customer_id").orderBy(F.col("ts").cast("timestamp").cast('long')).rangeBetween(-days(1), 0))
        res = df.withColumn("transactions_6hr_sum", F.count("ts").over(w_6h)) \
            .withColumn("transactions_1day_sum", F.count("ts").over(w_1d)) \
            .withColumn("spend_6hr_sum", F.sum("spend").over(w_6h)) \
            .withColumn("spend_1day_sum", F.sum("spend").over(w_1d)) \
            .select("customer_id", "timestamp", "transactions_6hr_sum", "transactions_1day_sum", "spend_6hr_sum", "spend_1day_sum")
        return res
    
```

The transform() method's input is actually the source wrapped in a dataframe. The transform method applies transformations (in this case, windowed aggregates) on the sources and returns the features as a dataframe. The output would look like this:

| customer_id | timestamp | transactions_6hr_sum  |transactions_1day_sum | spend_6hr_sum | spend_1day_sum|
|---------|-------------------------|--------| ---|--|--|
|01| 2022-07-31 23:11:00.000| 1|1 |25.53| 25.53|
|01| 2022-08-01 01:12:0.000|2|2|46.43|46.43|
|01| 2022-08-01 02:22:0.000|3|3|146.44|146.44|
|01| 2022-08-01 03:22:0.000|4|4|271.74|271.74|
|01| 2022-08-01 12:01:0.000|1|5|25.5|297.24|
|01| 2022-08-01 12:30:0.000|2|6|85.5|357.24|
|01| 2022-08-01 23:11:59.000|1|6|55.5|387.21|
|02| 2022-08-01 23:11:00.000|1|1|25.53|25.53|
|...| ...|... | ...|... |... |

### Step 2: Lookup/create observation data
For the samples you want in the training data, you need keys (index_columns) of entities, target (that is, what we are trying to predict) and timestamp column (of the target). We call this observation data. In our case, we need to have list of customer IDs for which churn needs to be calculated, the target (churned or not) and event timestamp. Sample data:

| customer_id | churn | event_timestamp  |
|------|-----|----|
01 | 0 | 2022-08-01 01:15:0.000 |
03 | 1 | 2022-08-01 03:30:0.000 |

### Step 3: Retrieve features data (training data prep)
Now you can add features to the observation data set to creating training data

```python
features = [customer_transaction_featureset["transactions_6hr_sum"], customer_demographics_featureset["income"],...]
training_data_df = feature_store.get_offline_features(features, observation_data_df, timestamp="event_timestamp")
```

The system does a point-of-tome join to create the training data:

| customer_id | churn | event_timestamp  | transactions_6hr_sum | income|
|------|-----|----|--|--|
01 | 0 | 2022-08-01 01:15:0.000 | 2 | 100000|
03 | 1 | 2022-08-01 03:30:0.000 |0 | 80000|

## Understanding point-of-time join 
A point-of-time join (also known as a temporal join) joins help address data leakage.

[Data leakage](https://en.wikipedia.org/wiki/Leakage_(machine_learning)) (also known as target leakage) is the use of information in the model training process that wouldn't be expected to be available at prediction time. Data leakage can cause the predictive scores (metrics) to overestimate the model's utility when run in a production environment. For more information on data leakage, see this article [data leakage](https://www.kaggle.com/code/alexisbcook/data-leakage#Target-leakage).

The below illustration explains how point-in-time joins in feature store works:

At time t0, label L0 was created and at t1, label L1 was created. Now feature values can change over time. When we create training sample for L0 and L1, what values of features feature1, feature2 and feature3 will we select? We have to select the values that correspond to t0 an t1.

:::image type="content" source="./media/concept-managed-feature-store/pit-join.png" alt-text="Diagram visualizing the selection of feature values at the appropriate point in time":::

The below table shows the output of get_offline_features method that does the point-of-time join:

:::image type="content" source="./media/concept-managed-feature-store/pit-join-output.png" alt-text="Diagram visualizing feature values output from a point-in-time join":::

## Next steps
<!-- Add a context sentence for the following links -->
- [Write concepts](contribute-how-to-write-concept.md)
- [Links](links-how-to.md)


