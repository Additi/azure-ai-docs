---
title: How to build a CICD data ingestion pipeline
titleSuffix: Azure Machine Learning
description: Learn how to use Azure Pipelines to automate the ingestion of data used to train your models. Add more info here that describes what the document talks about
services: machine-learning
ms.service: machine-learning
ms.subservice: core
ms.topic: conceptual
ms.author: csteegz
author: MayMSFT
manager: cgronlun
ms.reviewer: larryfr
ms.date: 01/15/2020

# Customer intent: As an experienced data scientist, I need to create a production data ingestion pipeline for the data used to train my models.

---

# DevOps for a Data Ingestion pipeline

In most scenarios a Data Ingestion solution is a composition of scripts, service invocations and a pipeline orchestrating all the activities. In this article, you learn how to apply DevOps practices to the development lifecycle of a common data ingestion pipeline taking care of the data preparation for the ML model training.

<!-- ## Prerequisites

* Bulleted list of things you need to be successful -->

## The solution

Consider the following data ingestion workflow:

In this approach, the training data is stored in a blob storage. An Azure Data Factory pipeline fetches the data from an input blob container, transforms it and saves the data to the output blob container serving as a data storage [LINK] for the Azure Machine Learning service. Having the data prepared, the Data Factory pipeline invokes a training Machine Learning pipeline to train a model. In this specific example the data fetching, transforming and saving to the output blob container is performed by a Python notebook, running on an Azure Databricks cluster.

## What we are building

As with any software solution, there is a team (e.g. Data Engineers) working on it. 

[Diagram]

They collaborate and share the same Azure resources such as Azure Data Factory workspace, Azure Databricks workspace, Azure Storage account, etc. The collection of these resources is a Development environment. The data engineers contribute to the same source code base. The Continuous Integration process assembles the code, checks it with the numerous code quality tests, unit tests and produces artifacts (tested code, ARM templates) ready to be deployed to the downstream environments by the Continuous Delivery process. This article demonstrates how to automate CI and CD processes with Azure DevOps (link) pipelines.

## Source Control Management

The team members work in slightly different ways to collaborate on the source code of the Python notebook and the source code of the Azure Data Factory pipeline. However, in both cases the code is stored in a source control repository (e.g. Azure DevOps, GitHub, GitLab, etc.) and the collaboration is normally based on some branching model (e.g. GitFlow[link]).

### Python Notebook Source Code

The data engineers work with the Python notebook source code either locally in a local IDE (e.g. VSCode[link]) or directly in the Databricks workspace leveraging the ability to debug the code on the development environment. In any case the code is going to be merged to the collaboration branch in the source control repository following a branching policy. It is highly recommended to store the code in regular *.py files rather than in Jupyter notebook format. It improves the code readability it enables automatic code quality checks in the CI process. 

### Azure Data Factory Source Code

The source code of Azure Data Factory pipelines is a bunch of json files generated by a workspace. Normally the data engineers work with the pipeline visual designer in the Azure Data Factory workspace rather than committing the source code directly. The workspace should be configured with a source control repository as it is described in the [Azure Data Factory documentation](https://docs.microsoft.com/en-us/azure/data-factory/source-control#author-with-azure-repos-git-integration) so it saves and reads the the source code from the repository. With this configuration in place the data engineers are able to collaborate on the source code following a preferred branching workflow.    

## CI

The ultimate goal of the Continuous Integration process is to gather the joint team work from the source code and prepare it to be deployed to the downstream environments. As with the source code management this process is different for the Python Notebooks and Azure Data Factory pipelines. 

### Python Notebook CI

The CI process for the Python Notebooks gets the code from the collaboration branch (e.g. ***master*** or ***develop***) and performs the following activities:
* Code linting
* Unit testing
* Save the code as an artifact 

The following code snippet demonstrates the implementation of these steps in an Azure DevOps ***yaml*** pipeline:

```yaml
steps:
- script: |
   flake8 --output-file=$(Build.BinariesDirectory)/lint-testresults.xml --format junit-xml  
  workingDirectory: '$(Build.SourcesDirectory)'
  displayName: 'Run flake8 (code style analysis)'  
  
- script: |
   python -m pytest --junitxml=$(Build.BinariesDirectory)/unit-testresults.xml $(Build.SourcesDirectory)
  displayName: 'Run unit tests'

- task: PublishTestResults@2
  condition: succeededOrFailed()
  inputs:
    testResultsFiles: '$(Build.BinariesDirectory)/*-testresults.xml'
    testRunTitle: 'Linting & Unit tests'
    failTaskOnFailedTests: true
  displayName: 'Publish linting and unit test results'

- publish: $(Build.SourcesDirectory)
    artifact: di-notebooks

```

The pipeline uses ***flake8*** to do the Python code linting, it runs the unit tests defined in the source code and publishes the linting and test results so they are available in the Azure DevOps pipeline execution screen:


If the linting and unit testing is successful, the pipeline will copy the source code to the artifact repository so it can be used by the subsequent deployment steps.

### Azure Data Factory CI

CI process for an Azure Data Factory pipeline is a ***bottleneck*** in the whole CI/CD story for a data ingestion pipeline. There is no actually ***Continuous*** Integration. A deployable artifact for Azure Data Factory is a collection of ARM templates. The only way to produce those templates is to click the ***publish*** button in the Azure Data Factory workspace. There is no automation here. The data engineers merge the source code from their feature branches into the collaboration branch (e.g. ***master*** or ***develop***) and then someone with the granted permissions clicks the ***publish*** button to generate ARM templates from the source code in the collaboration branch. When the button is clicked, the workspace validates the pipelines (think of it as of linting and testing), generates ARM templates (think of it as of building) and saves the generated templates to a technical branch ***adf_publish*** in the same code repository (think of it as of publishing artifacts). This branch is created automatically by the Azure Data Factory workspace. This process is described in details in the [Azure Data Factory documentation](https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment).

It is important to make sure that the generated ARM templates are environment agnostic, meaning that all values that may differ from environment to environment are parametrized. The Azure Data Factory is smart enough to expose the majority of such values as parameters. For example, in the following template the connection properties to a Machine Learning workspace are exposed as parameters:

```json
{
	"$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"value": "devops-ds-oh-adf"
		},
		"AzureMLService_servicePrincipalKey": {
			"value": ""
		},
		"AzureMLService_properties_typeProperties_subscriptionId": {
			"value": "0fe1c235-5cfa-4152-17d7-5dff45a8d4ba"
		},
		"AzureMLService_properties_typeProperties_resourceGroupName": {
			"value": "devops-ds-oh-rg"
		},
		"AzureMLService_properties_typeProperties_servicePrincipalId": {
			"value": "6e35e589-3b22-4edb-89d0-2ab7fc08d488"
		},
		"AzureMLService_properties_typeProperties_tenant": {
			"value": "72f988bf-86f1-41af-912b-2d7cd611db47"
		}
	}
}
```

However, you may want to expose your custom properties that are not handled by the Azure Data Factory workspace out-of-the-box. For example, in the scenario of this article  an Azure Data Factory pipeline invokes a Python notebook processing the data. The notebook accepts a parameter with a name of an input data file. 

Code

This name is different for ***Dev***, ***QA***, ***UAT*** and ***PROD*** environments. In a complex pipeline with a number of activities there can be plenty of values like that here and there. It's a good practice to collect all those values in one place and define them as pipeline ***variables***:

[Picture with pipeline variables]()


The pipeline activities may refer to the pipeline variables while actually using them:

[Picture with invoking a notebook]()

The Azure Data Factory workspace ***doesn't*** expose pipeline variables as ARM templates parameters by default. The workspace uses the [Default Parametrization Template](https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#default-parameterization-template) dictating what pipeline properties should be exposed as ARM template parameters. In order to add pipeline variables to the list, update the "Microsoft.DataFactory/factories/pipelines" section of the [Default Parametrization Template](https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment#default-parameterization-template) with the following snippet and place the result json file in the root of the source folder:

```json
"Microsoft.DataFactory/factories/pipelines": {
        "properties": {
            "variables": {
                "*": {
                    "defaultValue": "="
                }
            }
        }
    }
```

This will force the Azure Data Factory workspace to add the variables to the parameters list when the ***publish*** button is clicked:

```json
{
	"$schema": "https://schema.management.azure.com/schemas/2015-01-01/deploymentParameters.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"factoryName": {
			"value": "devops-ds-oh-adf"
		},
        ...
        "data-ingestion-pipeline_properties_variables_data_file_name_defaultValue": {
			"value": "porto_seguro_safe_driver_prediction_train.csv"
		}        
	}
}
```

The values in the json file are some default values configured in the pipeline definition and they are expected to be overridden with the target environment values when the ARM template is being deployed.

Building all together?????

## CD

The goal of the Continuous Delivery process is to take prepared by CI artifacts, deploy them to the first target environment, make sure that the solution works by running some tests, proceed to the next environment and so on. The Azure DevOps CD pipeline would consist of multiple stages representing the environments. Each stage contains [deployments]() and [jobs]() implementing the following steps:
* Deploy a Python Notebook to Azure Databricks workspace
* Deploy an Azure Data Factory pipeline 
* Run the pipeline
* Check the data ingestion result

The pipeline stages can be configured with [approvals] and [gates] providing additional control on how the deployment process evolves through the chain of environments.

### Deploy a Python Notebook

The following code snippet defines a [deployment]() deploying a Python Notebook to a Databricks cluster:

```yaml
- stage: 'Deploy_to_QA'
  displayName: 'Deploy to QA'
  variables:
  - group: devops-ds-oh-qa-vg
  jobs:
  - deployment: "Deploy_to_Databricks"
    displayName: 'Deploy to Databricks'
    timeoutInMinutes: 0
    environment: qa
    strategy:
      runOnce:
        deploy:
          steps:
            - task: UsePythonVersion@0
              inputs:
                versionSpec: '3.x'
                addToPath: true
                architecture: 'x64'
              displayName: 'Use Python3'

            - task: configuredatabricks@0
              inputs:
                url: '$(DATABRICKS_URL)'
                token: '$(DATABRICKS_TOKEN)'
              displayName: 'Configure Databricks CLI'    

            - task: deploynotebooks@0
              inputs:
                notebooksFolderPath: '$(Pipeline.Workspace)/di-notebooks'
                workspaceFolder: '/Shared/devops-ds-oh'
              displayName: 'Deploy (copy) data processing notebook to the Databricks cluster'       
```            

The artifacts produced by the CI are automatically copied to the deployment agent and available in the ***$(Pipeline.Workspace)*** folder. In this case the deployment task is referring to the ***di-notebooks*** artifact containing the Python Notebook. This [deployment]() uses the [Databricks Azure DevOps extension]() to copy the notebook files to the Databricks workspace. 
The ***Deploy_to_QA*** stage contains a reference to ***devops-ds-oh-qa-vg*** variable group defined in the Azure DevOps project. The steps in the stage are referring to the variables from this variable group (e.g. $(DATABRICKS_URL), $(DATABRICKS_TOKEN)). The idea is that the next stage (e.g. ***Deploy_to_UAT***) will operate with the same variable names but defined in its own UAT-scoped variable group. 

### Deploy an Azure Data Factory pipeline



## Next steps

* Links to documents that are logical next steps. Just a couple, maybe 3 or 4.
