---
title: Monitoring generative AI applications in production (preview)
titleSuffix: Azure Machine Learning
description: Monitor the safety and quality of generative AI applications deployed to production on Azure Machine Learning.
services: machine-learning
author: buchananwp
ms.author: wibuchan
ms.service: machine-learning
ms.subservice: prompt-flow
ms.reviewer: scottpolly
reviewer: s-polly
ms.topic: how-to
ms.date: 09/06/2023
ms.custom: devplatv2
---


# Monitoring generative AI applications in production

Monitoring models in production is an essential part of the AI lifecycle. Changes in data and consumer behavior can influence your LLM application over time, resulting in outdated AI systems  that negatively affect business outcomes and expose organizations to compliance, economic, and reputational risks. 

> [!IMPORTANT]
> Prompt flow is currently in public preview. This preview is provided without a service-level agreement, and is not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).

Azure Machine Learning model monitoring for generative AI applications makes it easier for you to monitor your LLM applications in production for safety and quality on a cadence to ensure it's delivering maximum business impact. Monitoring ultimately helps maintain the quality and safety of your generative AI applications. Capabilities and integrations include: 
- Collect production data using [Model data collector](../concept-data-collection.md).
- Key [responsible AI evaluation metrics](#understanding-evaluation-metrics) such as groundedness, coherence, fluency, relevance, and similarity, which are interoperable with [Azure Machine Learning prompt flow evaluation metrics](how-to-bulk-test-evaluate-flow.md).
- Ability to configure alerts for violations based on organizational targets and run monitoring on a recurring basis
- Consume results in a rich dashboard within a workspace in the Azure Machine Learning studio.
- Integration with Azure Machine Learning prompt flow evaluation metrics, analysis of collected production data to provide timely alerts, and visualization of the metrics over time. â€‹

For overall model monitoring basic concepts, refer to [Model monitoring with Azure Machine Learning (preview)](../concept-model-monitoring.md). In this article, you learn how to monitor a generative AI application backed by a managed online endpoint. The steps you take are:

- Configure [prerequisites](#prerequisites)
- [Create your monitor](#create-your-monitor)
- [Visualize metrics](#visualize-metrics)

> [!IMPORTANT]
> Monitoring is currently in public preview. This preview is provided without a service-level agreement, and are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).

## Understanding evaluation metrics 

### Evaluation metrics methodology
These metrics are generated by the following state-of-the-art GPT language models configured with specific evaluation instructions(prompt templates) which act as evaluators for sequence-to-sequence tasks. This technique has shown strong empirical results and high correlation with human judgment when compared to standard generative AI evaluation metrics. You can understand the generative AI monitoring metrics and use-cases in [Monitoring evaluation metrics descriptions and use cases](concept-model-monitoring-generative-ai-evaluation-metrics.md) and [Submit bulk test and evaluate a flow (preview)](how-to-bulk-test-evaluate-flow.md). The following GPT models are supported: 
* GPT-3.5 Turbo
* GPT-4
* GPT-4-32k  

> [!NOTE] 
> Running evaluations with Azure OpenAI resources will incur usage on your account. 

## Prerequisites
1. Create and configure Azure OpenAI resource. 
You must have an Azure OpenAI resource configured with a workspace connection. Follow the steps below to configure one. Your Azure OpenAI resource needs to be manually configured with access policies to have the appropriate permissions. Follow the instructions below:
1. Create a workspace connection [following this guidance](get-started-prompt-flow.md#connection). **DO NOT** delete the connection once it's used in the flow. You'll use a Managed Online Endpoints (MIR) User Assigned Identity (UAI)
    - This represents your Azure OpenAI resource backing your application
    - Grant permissions [to the endpoint identity](how-to-deploy-for-real-time-inference.md#grant-permissions-to-the-endpoint). If you use system-assigned identity, you need to assign "Azure Machine Learning Data Scientist" role of workspace to the endpoint identity. Use the following table to confirm you have the appropriate permissions. To learn more, see [Create and manage runtimes (preview)](how-to-create-manage-runtime.md).

    | Resource                  | Role                                  | Member | Why do I need this?                      |
    |---------------------------|---------------------------------------|------------------------------------------| ------------------------------------------|
    | Workspace                 | Azure Machine Learning Data Scientist | TODO | Used to write to run history, log metrics |
    | Workspace default ACR     | AcrPull                               | TODO | Pull image from ACR                      |
    | Workspace default storage | Storage Blob Data Contributor         | TODO | Write intermediate data and tracing data |
    | Workspace default storage | Storage Table Data Contributor        | TODO | Write intermediate data and tracing data |

> [!IMPORTANT]
> During preview, the correct permissions are required to configure your monitoring, or your monitor will fail. Please double-check your connection permissions.
 
1. Create a prompt flow runtime [following this guidance](how-to-create-manage-runtime.md) and clone a sample flow or create flow from scratch, specify the connection and runtime, and run it. 

1. Configure your flow to have both inputs and outputs configured. Minimum required parameters:
    - "prompt" (also known as "inputs" or "question")
    - "completion" (also known as "outputs" or "answer"). 

    :::image type="content" source="./media/monitor-generative-ai-applications/pf-configure-flow.png" alt-text="Screenshot showing how to configure prompt flow input and output settings for generative AI." lightbox="./media/how-to-monitor-generative-ai-applications/pf-configure-flow.png":::

1. After your prompt flow run successfully completes, select "Deploy" and finish the deploy wizard by [using this article as a guide](how-to-deploy-for-real-time-inference.md) 
    1. Capture outputs: ensure the required flow outputs are being captured: 
        - Required: "completion" (or "answer") 
        - Optional: "context" or "ground truth" 
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/pf-confirm-outputs.png" alt-text="Screenshot showing how to configure prompt flow outputs for data collection." lightbox="./media/how-to-monitor-generative-ai-applications/pf-confirm-outputs.png":::

    1. Enable inference data collection, which uses Azure Machine Learning's [Model Data Collector](../concept-data-collection.md).
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/pf-configure-data-collection.png" alt-text="Screenshot showing how to configure prompt flow data collection using Model Data Collector." lightbox="./media/how-to-monitor-generative-ai-applications/pf-configure-data-collection.png":::
    
    1. Enable your desired connection that you previously configured. You need to remember your connection and deployment names. 
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/pf-confirm-connection.png" alt-text="Screenshot showing how to configure prompt flow to your workspace connection" lightbox="./media/how-to-monitor-generative-ai-applications/pf-confirm-connection.png":::

    1. Confirm your settings
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/pf-confirm-settings.png" alt-text="Screenshot showing your configuration settings for confirmation." lightbox="./media/how-to-monitor-generative-ai-applications/pf-confirm-settings.png":::  
 
    The deployment creation may take more than 15 mins. 
1. After deployment creation finishes, confirm your deployment is fully functional using the 'test' tab of your endpoint
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/pf-confirm-deployment.png" alt-text="Screenshot showing your configured prompt flow deployment." lightbox="./media/how-to-monitor-generative-ai-applications/pf-confirm-deployment.png"::: 

1. Confirm data is being collected in the proper format of your data asset as specified above. Use a web-based json validator to confirm the structure before proceeding.
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/pf-confirm-data-collection.png" alt-text="Screenshot showing your data asset." lightbox="./media/how-to-monitor-generative-ai-applications/pf-confirm-data-collection.png":::  


## Create your monitor 

### Configure basic monitoring settings

Change **model task type** to **prompt & completion**, as shown by (A) in the screenshot.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-basic-settings.png" alt-text="Screenshot showing how to configure basic monitoring settings for generative AI." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-basic-settings.png":::

### Configure data asset

No action is required. Monitoring will automatically join your model inputs and outputs, but you can override with a custom data asset if desired. 
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-data-asset.png" alt-text="Screenshot showing how to configure your data asset for generative AI." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-data-asset.png":::
    
### Select monitoring signals

:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal2.png" alt-text="Screenshot showing monitoring signal configuration options on the monitoring settings dialog." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal2.png":::
1. Configure workspace connection **(A)** in the screenshot. 
    1. You need to configure your workspace connection correctly, or you see this: 
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal1.png" alt-text="Screenshot showing an unconfigured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal1.png":::
1. Enter your Azure OpenAI evaluator deployment name **(B)**.
1. (Optional) Join your production data inputs & outputs: your production model inputs and outputs are automatically joined by the Monitoring service **(C)**. You can customize this if needed, but no action is required. By default, the join column is **correlationid**, which is generated by Model Data Collector. 
1. (Optional) Configure metric thresholds: An acceptable per-instance score is fixed at 3/5. You can adjust your passing rate as-needed **(D)**.
        
#### Metric configuration requirements

The following inputs (data column names) are required to measure generation safety & quality: 

- **prompt text** - the original prompt given 
- **completion text** - the final completion from the API call that is returned
- **context text** - any context data that is sent to the API call, together with original prompt. For example, if you hope to get search results only from certain certified information sources/website, you can define in the evaluation steps. This is an optional step that can be configured through PromptFlow.
- **ground truth text** - the user-defined text as the "source of truth" (optional)

What parameters are configured in your data asset dictates what metrics you can produce. 

|Metric  |Prompt  |Completion  |Context  |Ground truth  |
|---------|---------|---------|---------|---------|
|Coherence     |Required        |Required        |    -     |    -     |
|Fluency     |Required        |Required         |     -    |      -   |
|Groundedness     |Required        |Required         |Required         |     -    |
|Relevance     |Required       |Required         |Required         |     -    |
|Similarity     |Required        |Required         |   -      |Required         |

- Manually enter column names from your prompt flow. Standard names are ("prompt" | "completion" | "context" | "ground_truth") but you can configure it according to your data asset
- (optional) Set sampling rate

- Once configured, your signal will no longer show a warning.
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal3.png" alt-text="Screenshot showing monitoring signal configurations without a warning." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal3.png":::

### Configure notifications

No action is required. You can configure more recipients if needed.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-notifications.png" alt-text="Screenshot showing monitoring notification configurations." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-notifications.png":::

### Confirm monitoring signal configuration  

When successfully configured, your monitor should look like this:
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-confirm-configuration.png" alt-text="Screenshot showing a configured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-confirm-configuration.png":::

### Confirm monitoring status

If successfully configured, your monitoring pipeline job shows the following:
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-confirm-job-success.png" alt-text="Screenshot showing a successfully configured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-confirm-job-success.png":::

### Visualize metrics  

#### Monitor overview page

Your monitor overview provides an overview of your signal performance. You can enter your signal details page for more information.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-monitor-overview.png" alt-text="Screenshot showing monitor overview." lightbox="./media/monitor-generative-ai-applications/how-to-gsq-monitor-overview.png":::
    
#### Signal details page

The signal details page allows you to view metrics over time **(A)**  as well as view histograms of distribution **(B)**.

:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-monitor-signal-details.png" alt-text="Screenshot showing a signal details page." lightbox="./media/monitor-generative-ai-applications/how-to-gsq-monitor-signal-details.png":::

#### Resolve alerts 

It's only possible to adjust signal thresholds. 
The acceptable score is fixed at 3/5; you can only adjust the 'acceptable overall % passing rate' field. 
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-monitor-signal-adjust-signal.png" alt-text="Screenshot adjusting signal thresholds." lightbox="./media/how-to-monitor-generative-ai-applications/how-to-gsq-monitor-signal-adjust-signal.png":::
   
## Next Steps

- [Get started with Prompt flow](get-started-prompt-flow.md)
- [Submit bulk test and evaluate a flow (preview)](how-to-bulk-test-evaluate-flow.md)
- [Create evaluation flows](how-to-develop-an-evaluation-flow.md)