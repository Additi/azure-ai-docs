---
title: Model monitoring for generative AI applications (preview)
titleSuffix: Azure Machine Learning
description: Monitor the safety and quality of generative AI applications deployed to production on Azure Machine Learning.
services: machine-learning
author: buchananwp
ms.author: wibuchan
ms.service: machine-learning
ms.subservice: prompt-flow
ms.reviewer: scottpolly
reviewer: s-polly
ms.topic: how-to
ms.date: 09/06/2023
ms.custom: devplatv2
---


# Model monitoring for generative AI applications (preview)

Monitoring models in production is an essential part of the AI lifecycle. Changes in data and consumer behavior can influence your generative AI application over time, resulting in outdated systems that negatively affect business outcomes and expose organizations to compliance, economic, and reputational risks. 

Azure Machine Learning model monitoring for generative AI applications makes it easier for you to monitor your LLM applications in production for safety and quality on a cadence to ensure it's delivering maximum business impact. Monitoring ultimately helps maintain the quality and safety of your generative AI applications. Capabilities and integrations include: 
- Collect production data using [Model data collector](../concept-data-collection.md).
- [Responsible AI evaluation metrics](#evaluation-metrics) such as groundedness, coherence, fluency, relevance, and similarity, which are interoperable with [Azure Machine Learning prompt flow evaluation metrics](how-to-bulk-test-evaluate-flow.md).
- Ability to configure alerts for violations based on organizational targets and run monitoring on a recurring basis
- Consume results in a rich dashboard within a workspace in the Azure Machine Learning studio.
- Integration with Azure Machine Learning prompt flow evaluation metrics, analysis of collected production data to provide timely alerts, and visualization of the metrics over time. ​

For overall model monitoring basic concepts, refer to [Model monitoring with Azure Machine Learning (preview)](../concept-model-monitoring.md). In this article, you learn how to monitor a generative AI application backed by a managed online endpoint. The steps you take are:

- Configure [prerequisites](#prerequisites)
- [Create your monitor](#create-your-monitor)
- [Confirm monitoring status](#confirm-monitoring-status)
- [Consume monitoring results](#consume-results)

> [!IMPORTANT]
> Monitoring and Promptflow features are currently in public preview. These previews are provided without a service-level agreement, and are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).



## How it works 
Metrics are generated by the following state-of-the-art GPT language models configured with specific evaluation instructions(prompt templates) which act as evaluator models for sequence-to-sequence tasks.  The following GPT models are supported, and will be configured as your Azure OpenAI resource: 
* GPT-3.5 Turbo
* GPT-4
* GPT-4-32k  


## Evaluation metrics
This technique has shown strong empirical results and high correlation with human judgment when compared to standard generative AI evaluation metrics. You can understand the generative AI monitoring metrics below, and learn more about [promptflow bulk test and evaluation (preview)](how-to-bulk-test-evaluate-flow.md). The following metrics are supported: 

- #### Coherence
    Evaluates how well the language model can produce output that is coherent: it flows smoothly, reads naturally, and resembles human-like language. Answers are scored in their clarity, brevity, linguistic quality, and ability to match defined user needs and expectations.  
    - **How to interpret:** 
        - **5 = “perfectly coherent”:** If the model's answers are highly coherent, it indicates that the AI system generates seamless, well-structured text with smooth transitions. Consistent context throughout the text enhances readability and understanding. 
        - **1 = “incoherent”:** Low coherence means that the quality of the sentences in a model's predicted answer is poor, and they do not fit together naturally. The generated text may lack a logical flow, and the sentences may appear disjointed, making it challenging for readers to understand the overall context or intended message.  
    - **Use-case:** You would like to test the readability and user-friendliness of your model's generated responses in real-world applications.

- #### Fluency
    Evaluates the language proficiency of a generative AI's predicted answer. It assesses how well the generated text adheres to grammatical rules, syntactic structures, and appropriate usage of vocabulary, resulting in linguistically correct and natural-sounding responses. Answers are measured by the quality of individual sentences, and whether are they well-written and grammatically correct. This metric is particularly valuable when evaluating the language model's ability to produce text that adheres to proper grammar, syntax, and vocabulary usage. 
    - **How to interpret:** 
        - 5 = ”perfect fluency”  If the model's answers are highly coherent, it indicates that the AI system follows grammatical rules and uses appropriate vocabulary. Consistent context throughout the text enhances readability and understanding. 
        - 1 = ”halting” low fluency scores indicate struggles with  grammatical errors and awkward phrasing, making the text less suitable for practical applications.  
    - **Use-case:** You would like to assess the grammatical and linguistic accuracy of the generative AI's predicted answers.

- #### Groundedness
    Evaluates how well the model's generated answers align with information from the input source. Answers are verified as claims against context in the user-defined ground truth source: even if answers are true (factually correct), if not verifiable against the source text, then it is scored as ungrounded. Responses verified as claims against “context” in the ground truth source (such as your input source or your database). 
    - **How to interpret:** 
        - 5 = "perfect groundedness": If the model's answers are highly grounded, it indicates that the facts covered in the AI system's responses are verifiable by the input source or internal database. 
        - 1 = "ungrounded": the facts mentioned in the AI system's responses may not be adequately supported or verifiable by the input source or internal database. 
    - **Use-case:** You are worried your application generates information that is not included as part of the your generative AI's trained knowledge (AKA unverifiable information).

- #### Relevance
    Evaluates the extent to which the model's generated responses are pertinent and directly related to the given questions. When users interact with a generative AI model, they pose questions or input prompts, expecting meaningful and contextually appropriate answers. Answers are scored in their ability to capture the key points of the question from the context in the ground truth source. 
    - **How to interpret:** 
        - 5 ="perfect relevance" If the model's answers are highly relevant, it indicates contextually appropriate outputs. 
        - 1 = "irrelevant" Conversely, low relevance scores suggest that the generated responses might be off-topic, lack context, or fail to address the user's intended queries adequately.    
    - **Use-case:** You would like to achieve high relevance for your application's answers to enhance the user experience and utility of your generative AI systems.
    
- #### Similarity 
    Similarity quantifies the similarity between a ground truth sentence (or document) and the prediction sentence generated by an AI model. It is calculated by first computing sentence-level embeddings for both the ground truth and the model's prediction=, which represent high-dimensional vector representations of the sentences, capturing their semantic meaning and context. Answers are scored for equivalencies to the ground-truth answer by capturing the same information and meaning as the ground-truth answer for the given question.
    - **How to interpret:** 
        - 5="perfect equivalence" suggests that the model's prediction is very contextually similar to the ground truth, indicating accurate and relevant results. 
        - 1= "non-equivalence" a low similarity score implies a mismatch or divergence between the prediction and the actual ground truth, potentially signaling inaccuracies or deficiencies in the model's performance.
    - **Use-case:** You would like to objectively evaluate the performance of an AI model (for text generation tasks where you have access to ground truth desired responses). Ada similarity allows you to compare the generated text against the desired content.
    
#### Metric configuration requirements

The following inputs (data column names) are required to measure generation safety & quality: 

- **prompt text** - the original prompt given 
- **completion text** - the final completion from the API call that is returned
- **context text** - any context data that is sent to the API call, together with original prompt. For example, if you hope to get search results only from certain certified information sources/website, you can define in the evaluation steps. This is an optional step that can be configured through PromptFlow.
- **ground truth text** - the user-defined text as the "source of truth" (optional)

What parameters are configured in your data asset dictates what metrics you can produce, according to this table:  

|Metric  |Prompt  |Completion  |Context  |Ground truth  |
|---------|---------|---------|---------|---------|
|Coherence     |Required        |Required        |    -     |    -     |
|Fluency     |Required        |Required         |     -    |      -   |
|Groundedness     |Required        |Required         |Required         |     -    |
|Relevance     |Required       |Required         |Required         |     -    |
|Similarity     |Required        |Required         |   -      |Required         |


## Prerequisites
1. **Azure OpenAI resource:** You must have an Azure OpenAI resource configured with the appropriate access policies outlined below.
1. **Workspace connection:** [following this guidance](get-started-prompt-flow.md#connection), you'll use a managed identity which will represent the credentials to the Azure OpenAI endpoint leveraged to calculate the monitoring metrics. It will need permissions to be configured according to the following table. To learn more, see [Create and manage runtimes (preview)](how-to-create-manage-runtime.md). **DO NOT** delete the connection once it's used in the flow.
1. **Promptflow deployment:** Create a prompt flow runtime [following this guidance](how-to-create-manage-runtime.md), run your flow, and ensure your [deployment is configured using this article as a guide](how-to-deploy-for-real-time-inference.md) 
    - **Flow inputs & outputs:** in this article, we'll use the following:
        - **Inputs:** "prompt" (also known as "inputs" or "question")
        - **Outputs:** "completion" (also known as "outputs" or "answer") with optional parameters: "context" or "ground truth" 
    - **Data collection:** 'inference data collection' must be enabled using [Model Data Collector](../concept-data-collection.md) and needs model inputs ("prompt") and outputs ("completion") columns.

## Create your monitor 
Create your monitor in the Monitoring overview page 
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-create-monitor.png" alt-text="Screenshot showing how to configure basic monitoring settings for generative AI." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-create-monitor.png":::


### Configure basic monitoring settings

In the monitoring creation wizard, change **model task type** to **prompt & completion**, as shown by (A) in the screenshot.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-basic-settings.png" alt-text="Screenshot showing how to configure basic monitoring settings for generative AI." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-basic-settings.png":::

### Configure data asset

If you have used [Model Data Collector](../concept-data-collection.md), select your two data assets (inputs & outputs). 
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-data-asset.png" alt-text="Screenshot showing how to configure your data asset for generative AI." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-data-asset.png":::
    
### Select monitoring signals

:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal2.png" alt-text="Screenshot showing monitoring signal configuration options on the monitoring settings dialog." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal2.png":::
1. Configure workspace connection **(A)** in the screenshot. 
    1. You need to configure your workspace connection correctly, or you see this: 
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal1.png" alt-text="Screenshot showing an unconfigured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal1.png":::
1. Enter your Azure OpenAI evaluator deployment name **(B)**.
1. (Optional) Join your production data inputs & outputs: your production model inputs and outputs are automatically joined by the Monitoring service **(C)**. You can customize this if needed, but no action is required. By default, the join column is **correlationid**. 
1. (Optional) Configure metric thresholds: An acceptable per-instance score is fixed at 3/5. You can adjust your acceptable overall % passing rate between the range [1,99] % 

- Manually enter column names from your prompt flow **(E)**. Standard names are ("prompt" | "completion" | "context" | "ground_truth") but you can configure it according to your data asset. 
- (optional) Set sampling rate **(F)**

- Once configured, your signal will no longer show a warning.
    :::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal3.png" alt-text="Screenshot showing monitoring signal configurations without a warning." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-signal3.png":::

### Configure notifications

No action is required. You can configure more recipients if needed.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-configure-notifications.png" alt-text="Screenshot showing monitoring notification configurations." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-configure-notifications.png":::

### Confirm monitoring signal configuration  

When successfully configured, your monitor should look like this:
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-confirm-configuration.png" alt-text="Screenshot showing a configured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-confirm-configuration.png":::

## Confirm monitoring status

If successfully configured, your monitoring pipeline job shows the following:
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-confirm-job-success.png" alt-text="Screenshot showing a successfully configured monitoring signal." lightbox="./media/how-to-monitor-generative-ai-applications/gsq-confirm-job-success.png":::

## Consume results  

### Monitor overview page

Your monitor overview provides an overview of your signal performance. You can enter your signal details page for more information.
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-monitor-overview.png" alt-text="Screenshot showing monitor overview." lightbox="./media/monitor-generative-ai-applications/how-to-gsq-monitor-overview.png":::
    
### Signal details page

The signal details page allows you to view metrics over time **(A)**  as well as view histograms of distribution **(B)**.

:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-monitor-signal-details.png" alt-text="Screenshot showing a signal details page." lightbox="./media/monitor-generative-ai-applications/how-to-gsq-monitor-signal-details.png":::

### Resolve alerts 

It's only possible to adjust signal thresholds: the acceptable score is fixed at 3/5, and it is only possible to adjust the 'acceptable overall % passing rate' field. 
:::image type="content" source="./media/how-to-monitor-generative-ai-applications/gsq-monitor-signal-adjust-signal.png" alt-text="Screenshot adjusting signal thresholds." lightbox="./media/how-to-monitor-generative-ai-applications/how-to-gsq-monitor-signal-adjust-signal.png":::
   
## Next Steps

- [Model data collector](../concept-data-collection.md)
- [Get started with Prompt flow](get-started-prompt-flow.md)
- [Submit bulk test and evaluate a flow (preview)](how-to-bulk-test-evaluate-flow.md)
- [Create evaluation flows](how-to-develop-an-evaluation-flow.md)