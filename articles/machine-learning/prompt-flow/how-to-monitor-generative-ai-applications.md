---
title: Monitoring generative AI applications in production (preview)
titleSuffix: Azure Machine Learning
description: Monitor the safety and quality of generative AI applications deployed to production on Azure Machine Learning.
services: machine-learning
author: buchananwp
ms.author: wibuchan
ms.service: machine-learning
ms.subservice: mlops
ms.reviewer: scottpolly
reviewer: s-polly
ms.topic: how-to
ms.date: 09/06/2023
ms.custom: devplatv2
---


# Monitoring generative AI applications in production (preview)

Monitoring models in production is an essential part of the AI lifecycle: changes in data and consumer behavior can influence your LLM application over time, resulting in outdated AI systems, which can produce undesired results that can negatively impact business outcomes and expose organizations to compliance, economic, and reputational risks. 

Azure Machine Learning model monitoring for generative AI applications makes it easier for you to monitor your LLM applications in production for safety and quality on a cadence to ensure it is delivering maximum business impact. This ultimately helps maintain the quality and safety of your generative AI applications. Capabilities and integrations include: 
- Collect production data using [Model Data Collector](concept-data-collection.md) 
- Key [responsible AI evaluation metrics](#understanding-evaluation-metrics) such as groundedness, coherence, fluency, relevance, and similarity, which are interoperable with [Azure Machine Learning prompt flow evaluation metrics](prompt-flow/how-to-bulk-test-evaluate-flow.md).
- Ability to configure alerts for violations based on organizational targets and run monitoring on a recurring basis
- Consume results in a rich dashboard within a workspace in the Azure Machine Learning studio.
- Integration with Azure Machine Learning prompt flow evaluation metrics, analysis of collected production data to provide timely alerts, and visualization of the metrics over time. â€‹

For overall model monitoring basic concepts, please refer to [Model monitoring with Azure Machine Learning (preview)](concept-model-monitoring.md). In this article, you'll learn how to monitor a generative AI application backed by a managed online endpoint. The steps you'll take are:

- [Configure prerequisites](#prerequisites)
- [Create your monitor](#create-your-monitor)
- [Visualize metrics in Studio UI](#visualize-metrics-in-studio-ui)

> [!IMPORTANT]
> Monitoring is currently in public preview. This preview is provided without a service-level agreement, and are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).

## Understanding evaluation metrics 

### Evaluation metrics methodology
These metrics are generated by the following state-of-the-art GPT language models configured with specific evaluation instructions(prompt templates) which act as evaluators for sequence-to-sequence tasks. This technique has shown strong empirical results and high correlation with human judgement when compared to standard generative AI evaluation metrics. You can [understand the generative AI monitoring metrics and use-cases here](../prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics.md) and [learn more about the full suite of AzureML prompt flow evaluation metrics here](../prompt-flow/how-to-bulk-test-evaluate-flow.md). The following GPT models are supported: 
* GPT-3.5 Turbo
* GPT-4
* GPT-4-32k  

> [!NOTE] 
> Running evaluations with Azure OpenAI resources will incur usage on your account. 

## Prerequisites
1. Create and configure Azure OpenAI resource 
You must have an Azure OpenAI resource configured with a workspace connection. Follow the steps below to configure one. Your Azure OpenAI resource will need to be manually configured with access policies to have the appropriate permissions. Follow the instructions below:
1. Create a workspace connection [following this guidance](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/get-started-prompt-flow?view=azureml-api-2#connection). **DO NOT** delete the connection once it's used in the flow. You will use a Managed Online Endpoints (MIR) User Assigned Identity (UAI)
- This will represent your Azure OpenAI resource backing your application
- Grant permissions [following this guidance](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-deploy-for-real-time-inference?view=azureml-api-2#grant-permissions-to-the-endpoint) to the endpoint identity. If you use system-assigned identity, you need to assign "AzureML Data Scientist" role of workspace to the endpoint identity. Use the following table to confirm you have the appropriate permissions. To learn more, [use this guidance](how-to-create-manage-runtime.md)

    | Resource                  | Role                                  | Member | Why do I need this?                      |
    |---------------------------|---------------------------------------|------------------------------------------| ------------------------------------------|
    | Workspace                 | Azure Machine Learning Data Scientist | TODO | Used to write to run history, log metrics |
    | Workspace default ACR     | AcrPull                               | TODO | Pull image from ACR                      |
    | Workspace default storage | Storage Blob Data Contributor         | TODO | Write intermediate data and tracing data |
    | Workspace default storage | Storage Table Data Contributor        | TODO | Write intermediate data and tracing data |

> [!IMPORTANT]
> During preview, the correct permissions are required to configure your monitoring, or your monitor will fail. Please double-check your connection permissions.
 
1. Create a promptflow runtime [following this guidance](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-create-manage-runtime?view=azureml-api-2) and clone a sample flow or create flow from scratch, specify the connection and runtime, run it. 

1. Configure your flow to have both inputs and outputs configured. Minimum required parameters:
    - "prompt" (aka "inputs" aka "question")
    - "completion" (aka "outputs" aka "answer"). 

    :::image type="content" source="./media/monitor-generative-ai-applications/pf-configure-flow.png" alt-text="Screenshot showing how to configure prompt flow input and output settings for generative AI." lightbox="./media/monitor-generative-ai-applications/pf-configure-flow.png":::

1. After your promptflow run successfully completes, select "Deploy" and finish the deploy wizard by [following this guidance](https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-deploy-for-real-time-inference?view=azureml-api-2) 
    1. Capture outputs: ensure the required flow outputs are being captured: 
        - Required: "completion" (aka "answer") 
        - Optional: "context" or "ground truth" 
    :::image type="content" source="./media/monitor-generative-ai-applications/pf-confirm-outputs.png" alt-text="Screenshot showing how to configure prompt flow outputs for data collection." lightbox="./media/monitor-generative-ai-applications/pf-confirm-outputs.png":::

    1. Enable inference data collection, which uses AzureML's [Model Data Collector](https://learn.microsoft.com/en-us/azure/machine-learning/concept-data-collection?view=azureml-api-2)
    :::image type="content" source="./media/monitor-generative-ai-applications/pf-configure-data-collection.png" alt-text="Screenshot showing how to configure prompt flow data collection using Model Data Collector." lightbox="./media/monitor-generative-ai-applications/pf-configure-data-collection.png":::
    
    1. Enable your desired connection that you previously configured. Note that you will need to remember your connection and deployment names. 
    :::image type="content" source="./media/monitor-generative-ai-applications/pf-confirm-connection.png" alt-text="Screenshot showing how to configure prompt flow to your workspace connection" lightbox="./media/monitor-generative-ai-applications/pf-confirm-connection.png":::

    1. Confirm your settings
    :::image type="content" source="./media/monitor-generative-ai-applications/pf-confirm-settings.png" alt-text="Screenshot showing your configured promptflow deployment." lightbox="./media/monitor-generative-ai-applications/pf-confirm-settings.png":::  
 
1. The deployment creation may take more than 15 mins. 
    1. After deployment creation finishes, confirm your deployment is fully functional using the 'test' tab of your endpoint
    :::image type="content" source="./media/monitor-generative-ai-applications/pf-confirm-deployment.png" alt-text="Screenshot showing your configured promptflow deployment." lightbox="./media/monitor-generative-ai-applications/pf-confirm-deployment.png"::: 

    1. Confirm data is being collected in the proper format of your data asset as specified above. Please use a web-based json validator to confirm the structure before proceeding.
    :::image type="content" source="./media/monitor-generative-ai-applications/pf-confirm-data-collection.png" alt-text="Screenshot showing your data asset." lightbox="./media/monitor-generative-ai-applications/pf-confirm-data-collection.png":::  


## Create your monitor 

### Configure basic monitoring settings
A. Change 'model task type' to 'prompt & completion'
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-configure-basic-settings.png" alt-text="Screenshot showing how to configure basic monitoring settings for generative AI." lightbox="./media/monitor-generative-ai-applications/gsq-configure-basic-settings.png":::

### Configure data asset
No action is required. Monitoring will automatically join your model inputs and outputs, but you can override with a custom data asset if desired. 
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-configure-data-asset.png" alt-text="Screenshot showing how to configure your data asset for generative AI." lightbox="./media/monitor-generative-ai-applications/gsq-configure-data-asset.png":::
    
### Select monitoring signals
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-configure-signal2.png" alt-text="Screenshot showing monitoring signal configurations." lightbox="./media/monitor-generative-ai-applications/gsq-configure-signal2.png":::
- A. Configure workspace connection 
    You will need to configure your workspace connection correctly, or you will see this: 
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-configure-signal1.png" alt-text="Screenshot showing an unconfigured monitoring signal." lightbox="./media/monitor-generative-ai-applications/gsq-configure-signal1.png":::
- B. Enter your Azure OpenAI evaluator deployment name (Note: this is a preview feature and will be automated)
- C. (optional) join your production data inputs & outputs: your production model inputs and outputs will be automatically joined by the Monitoring service. You can customize this if needed, but no action is required. By default, the join column is 'correlationid' which is generated by Model Data Collector. 
- D. (optional) Configure metric thresholds: An acceptable per-instance score is fixed at 3/5. You can adjust your passing rate as-needed.
        
#### Metric configuration requirements
The following inputs (data column names) are required to measure generation safety & quality: 

- **prompt (aka question) text** - the original prompt given 
- **completion (aka answer) text** - the final completion from the API call that is returned
- **context text** - any context data that is sent to the API call, together with original prompt. For example, if you hope to get search results only from certain certified information sources/website, you can define in the evaluation steps. This is an optional step that can be configured through PromptFlow.
- **ground truth text** - the user-defined text as the "source of truth" (optional)

What parameters are configured in your data asset will dictate what metrics you can produce. 
| Metric | Prompt  | Completion |  Context | Ground truth |
| -- | -- | -- | -- | -- | 
| Coherence  | Required | Required | -- |  -- |  
| Fluency | Required | Required | -- | -- |  
| Groundedness | Required | Required | Required | -- |  
| Relevance | Required | Required | Required | -- |  
| Similarity | Required | Required | -- | Required |  

- E. Manually enter column names from your prompt flow. Standard names are ("prompt" | "completion" | "context" | "ground_truth") but you can configure it according to your data asset
- F. (optional) Set sampling rate

- Once configured, your signal will no longer show a warning.
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-configure-signal3.png" alt-text="Screenshot showing monitoring signal configurations." lightbox="./media/monitor-generative-ai-applications/gsq-configure-signal3.png":::

### Configure notifications
No action is required. You can configure additional recipients if needed.
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-configure-notifications.png" alt-text="Screenshot showing monitoring notification configurations." lightbox="./media/monitor-generative-ai-applications/gsq-configure-notifications.png":::

### Confirm monitoring signal configuration  
When successfully configured, your monitor should look like this:
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-confirm-configuration.png" alt-text="Screenshot showing a configured monitoring signal." lightbox="./media/monitor-generative-ai-applications/gsq-confirm-configuration.png":::

### Confirm monitoring status
If successfully configured, your monitoring pipeline job will show the following:
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-confirm-job-success.png" alt-text="Screenshot showing a configured monitoring signal." lightbox="./media/monitor-generative-ai-applications/gsq-confirm-job-success.png":::

### Visualize metrics  
#### Monitor overview page
Your monitor overview will provide an overview of your signal performance. You can enter your signal details page for more information.
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-monitor-overview.png" alt-text="Screenshot showing monitor overview." lightbox="./media/monitor-generative-ai-applications/gsq-monitor-overview.png":::
    
#### Signal details page
A. View metrics over time

B. View histogram of distributions 
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-monitor-signal-details.png" alt-text="Screenshot showing a signal details page." lightbox="./media/monitor-generative-ai-applications/gsq-monitor-signal-details.png":::

#### Resolve alerts 
It is only possible to adjust signal thresholds. 
The acceptable score is fixed at 3/5; you can only adjust the 'acceptable overall % passing rate' field. 
:::image type="content" source="./media/monitor-generative-ai-applications/gsq-monitor-signal-adjust-signal.png" alt-text="Screenshot adjusting signal thresholds." lightbox="./media/monitor-generative-ai-applications/gsq-monitor-signal-adjust-signal.png":::
   