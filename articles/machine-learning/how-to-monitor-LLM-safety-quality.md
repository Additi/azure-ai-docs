---
how-to-monitor-LLM-safety-quality---
title: Monitoring generative AI applications in production (preview)
titleSuffix: Azure Machine Learning
description: Monitor the safety and quality of generative AI deployed to production on Azure Machine Learning.
services: machine-learning
author: buchananwp
ms.author: wibuchan
ms.service: machine-learning
ms.subservice: mlops
ms.reviewer: mopeakande
reviewer: msakande
ms.topic: conceptual
ms.date: 09/06/2023
ms.custom: devplatv2
---

# Overview - TO REFINE
Monitoring models in production is an essential part of the AI lifecycle: changes in data and consumer behavior can influence your LLM application over time, resulting in outdated AI systems, which can produce undesired results that can negatively impact business outcomes and expose organizations to compliance and reputational risks. 

AzureML model monitoring for generative AI applications makes it easier for you to monitor your LLM applications in production for safety and quality on a user-defined cadence to ensure it is delivering maximum business impact. This ultimately helps you maintain the quality and safety of your deployments, mitigating economic, reputational, and compliance risks. Capabilities: 
- Collect production data using Model Data Collector(https://learn.microsoft.com/en-us/azure/machine-learning/concept-data-collection?view=azureml-api-2), 
- Evaluation metrics include key responsible AI indicators such as groundedness, coherence, fluency, relevance, and similarity, and are interoperable with AzureML prompt flow. ​
- You can configure alerts for violations based on organizational targets and run your monitoring on a recurring basis
- Consume results in a rich dashboard within a workspace in the Azure Machine Learning studio.
- The service integrates with AzureML prompt flow evaluation metrics, analyzes collected production data to provide timely alerts, and allows for rich visualization of the metrics over time. ​

For overall model monitoring basic concepts, please refer to the model monitoring concept article: https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2

> [!IMPORTANT]
> Monitoring is currently in public preview. This preview is provided without a service-level agreement, and are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).


# Prerequisites 
1. Required data items to flow level "inputs" aka "prompt" aka "question aka "outputs" aka "completion" aka "response"
1. Data collection: enable MDC at deployment step.
1. Workspace connection & UAI: You will need to configure a workspace connection for your Azure OpenAI resource to have appropriate permissions. Learn more here.
(aka.ms/uaiwcconfig)

# Supported evaluation model types
Generative AI monitoring uses the GPT family of models as evaluators for sequence-to-sequence tasks. This technique has shown encouraging empirical results and high correlation with human judgement when compared to standard evaluation metrics. These metrics are generated by the following state-of-the-art GPT language models: 
- GPT-3.5 Turbo
- GPT-4
- GPT-4-32k  

# Evaluation metrics 
You can learn more here: articles/machine-learning/concept-evaluation-metrics-guidance.md

## Metrics requirements
- The following inputs (data column names) are required to measure generation safety & quality: 
* **prompt (aka question) text** - the original prompt given by user
* **completion (aka answer) text** - the final completion from LLM API call that is returned to user
* **context text** - any context data that is sent to LLM API call together with original prompt. This can be configured through PromptFlow. (optional)
* **ground truth text** - the user-defined text as the source of truth (optional)
- other part is the context/desired results.  For example, if a user/customer hopes to get search results only from certain certified information sources/website, the user/customer can define in the evaluation steps. 

# Metric configuration
What parameters are configured in your data asset will dictate what metrics you can produce. 
| Metric | Prompt  | Completion |  Context | Ground truth |
| -- | -- | -- | -- | -- | 
| Coherence  | Required | Required | -- |  -- |  
| Fluency | Required | Required | -- | -- |  
| Groundedness | Required | Required | Required | -- |  
| Relevance | Required | Required | Required | -- |  
| Similarity | Required | Required | -- | Required |  

# Getting started
Define GPT-4 endpoint for calculating hallucination  
    - Attach workspace UAI with access to the AOAI resource 
    - API connection in workspace (should be able to represent Batch endpoint) 
    - If no workspace connection, alert and provide prompt to guide users to create one 
    - Configuring permissions
Create an online endpoint (promptflow)	  
    - Specifies “flow outputs” to collect production data ( Prompt| Completion | Context | Ground truth) 
    - Put into dataframe and log it, joining inputs and outputs  
    - Create PF deployment  
Create Model monitor 
    - Select deployed GPT4 LLM annotator model to create your monitoring signal  
    - Select target LLM output dataset (inputs & outputs ) 
    - Select metrics 
Consume/view metric in Studio UI 
    - View the metrics over time 
    - View histogram of distributions 
    - View samples of violations 