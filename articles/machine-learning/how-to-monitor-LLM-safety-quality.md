---
how-to-monitor-LLM-safety-quality---
title: Monitoring generative AI applications in production (preview)
titleSuffix: Azure Machine Learning
description: Monitor the safety and quality of generative AI deployed to production on Azure Machine Learning.
services: machine-learning
author: buchananwp
ms.author: wibuchan
ms.service: machine-learning
ms.subservice: mlops
ms.reviewer: mopeakande
reviewer: msakande
ms.topic: conceptual
ms.date: 09/06/2023
ms.custom: devplatv2
---

# Overview - TO REFINE
Monitoring models in production is an essential part of the AI lifecycle: changes in data and consumer behavior can influence your LLM application over time, resulting in outdated AI systems, which can produce undesired results that can negatively impact business outcomes and expose organizations to compliance and reputational risks. 

AzureML model monitoring for generative AI applications makes it easier for you to monitor your LLM applications in production for safety and quality on a user-defined cadence to ensure it is delivering maximum business impact. This ultimately helps you maintain the quality and safety of your deployments, mitigating economic, reputational, and compliance risks. Capabilities: 
- Collect production data using Model Data Collector(https://learn.microsoft.com/en-us/azure/machine-learning/concept-data-collection?view=azureml-api-2), 
- Evaluation metrics include key responsible AI indicators such as groundedness, coherence, fluency, relevance, and similarity, and are interoperable with AzureML prompt flow. ​
- You can configure alerts for violations based on organizational targets and run your monitoring on a recurring basis
- Consume results in a rich dashboard within a workspace in the Azure Machine Learning studio.
- The service integrates with AzureML prompt flow evaluation metrics, analyzes collected production data to provide timely alerts, and allows for rich visualization of the metrics over time. ​

For overall model monitoring basic concepts, please refer to the model monitoring concept article: https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2

> [!IMPORTANT]
> Monitoring is currently in public preview. This preview is provided without a service-level agreement, and are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).


# Prerequisites 
1. Required data items to flow level "inputs" aka "prompt" aka "question aka "outputs" aka "completion" aka "response"
1. Data collection: enable MDC at deployment step.
1. Workspace connection & UAI: You will need to configure a workspace connection for your Azure OpenAI resource to have appropriate permissions. Learn more here.
(aka.ms/uaiwcconfig)

# Supported evaluation model types
Generative AI monitoring uses the GPT family of models as evaluators for sequence-to-sequence tasks. This technique has shown encouraging empirical results and high correlation with human judgement when compared to standard evaluation metrics. These metrics are generated by the following state-of-the-art GPT language models: 
- GPT-3.5 Turbo
- GPT-4
- GPT-4-32k  

# Evaluation metrics 
You can learn more here: articles/machine-learning/concept-evaluation-metrics-guidance.md

## Metrics requirements
- The following inputs (data column names) are required to measure generation safety & quality: 
* **prompt (aka question) text** - the original prompt given by user
* **completion (aka answer) text** - the final completion from LLM API call that is returned to user
* **context text** - any context data that is sent to LLM API call together with original prompt. This can be configured through PromptFlow. (optional)
* **ground truth text** - the user-defined text as the source of truth (optional)
- other part is the context/desired results.  For example, if a user/customer hopes to get search results only from certain certified information sources/website, the user/customer can define in the evaluation steps. 

# Metric configuration
What parameters are configured in your data asset will dictate what metrics you can produce. 
| Metric | Prompt  | Completion |  Context | Ground truth |
| -- | -- | -- | -- | -- | 
| Coherence  | Required | Required | -- |  -- |  
| Fluency | Required | Required | -- | -- |  
| Groundedness | Required | Required | Required | -- |  
| Relevance | Required | Required | Required | -- |  
| Similarity | Required | Required | -- | Required |  

# Prerequisite: PromptFlow setup
create connection following this guidance. DO NOT delete the connection once it's used in the flow. 
- https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/get-started-prompt-flow?view=azureml-api-2#connection
create runtime following this guidance
- https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-create-manage-runtime?view=azureml-api-2
clone a sample flow or create flow from scratch, specify the connection and runtime, run it
After run successfully, select "Deploy" and finish the deploy wizard by following this guidance.
- https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-deploy-for-real-time-inference?view=azureml-api-2 
Remember to grant permissions to the endpoint identity. If you use system-assigned identity, you need to assign "AzureML Data Scientist" role of workspace to the endpoint identity.
- https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-deploy-for-real-time-inference?view=azureml-api-2#grant-permissions-to-the-endpoint
The deployment creation may take more than 15 mins. After deployment creation finishes, you can test it in endpoint detail page UI.



# Getting started
1. Create an Azure OpenAI resource which will be used as your evaluation endpoint    
1. Create a User Assigned Managed Identity (UAI) 
    - You need to assign enough permission. To assign a role, you need to have owner or have Microsoft.Authorization/roleAssignments/write permission on resource.
1. Grant permission to your UAI for your Azure OpenAI resource 
    - Owner role should be assigned. Confirm this in your Access control of your portal
    - In your resource group key vault, create an access policy with 'list secret' permissions to your UAI
1. Create a connection to your evaluation endpoint  
    - Attach workspace UAI with the correct access to the AOAI resource 
    - Add your API connection in workspace (this will represent your AOAI endpoint) 
    - If no workspace connection, alert and provide prompt to guide users to create one 
    - Configuring permissions
1. Configure your PromptFlow application
    1. inputs & outputs 
    1. Deploy your promptflow application
1. Ensure your authentication uses key-based auth and user-assigned ID. 
    4.1. choose your subscription and user-assigned identity 
1. Enable inferencing data collection (Model Data Collector) 
1. Enable your desired outputs (flow outputs) in the endpoint response ( Prompt| Completion | Context | Ground truth) 
1. Choose your Azure OpenAI connection and deployment name 
1. Review parameters and deploy
1. Confirm data collection is working 
1. Create Model monitor 
    - Select deployed GPT4 LLM annotator model to create your monitoring signal  
    - Select target LLM output dataset (inputs & outputs ) 
    - Select metrics 
1. Consume/view metric in Studio UI 
    - View the metrics over time 
    - View histogram of distributions 
    - View samples of violations 