---
how-to-monitor-LLM-safety-quality---
title: Monitoring models in production (preview)
titleSuffix: Azure Machine Learning
description: Monitor the safety and quality of generative AI deployed to production on Azure Machine Learning.
services: machine-learning
author: buchananwp
ms.author: wibuchan
ms.service: machine-learning
ms.subservice: mlops
ms.reviewer: mopeakande
reviewer: msakande
ms.topic: conceptual
ms.date: 09/06/2023
ms.custom: devplatv2
---

# Overview
High-level description about what can be monitored, i.e. generation safety and quality and its metrics.

Monitoring models in production is an essential part of the AI lifecycle. Changes in data and consumer behavior can influence your LLM application over time, resulting in outdated AI systems, which can produce undesired results that can negatively impact business outcomes and expose organizations to compliance and reputational risks. Azure Machine Learning now lets your organization monitor your LLM applications in production. 

You can collect production data using Model Data Collector(https://learn.microsoft.com/en-us/azure/machine-learning/concept-data-collection?view=azureml-api-2), analyze key safety & quality evaluation metrics on a recurring basis, receive timely alerts about critical issues, and visualize the results over time in a rich dashboard within the Azure Machine Learning studio 

For overall model monitoring basic concepts, please refer to the model monitoring concept article: https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2


# Prerequisites 
1. Required data items to flow level "inputs" aka "prompt" aka "question aka "outputs" aka "completion" aka "response"
1. Data collection: enable MDC at deployment step.
1. Workspace connection & UAI: You will need to configure a workspace connection for your Azure OpenAI resource to have appropriate permissions. Learn more here.
aka.ms/uaiwcconfig

# Supported evaluation model types
Generative AI monitoring uses the GPT family of models as evaluators for sequence-to-sequence tasks. This technique has shown encouraging empirical results and high correlation with human judgement when compared to standard evaluation metrics. These metrics are intended to be generated by state-of-the-art GPT language models: 
- GPT-3.5 Turbo
- GPT-4
- GPT-4-32k  

# Evaluation metrics: 
## Evaluation metrics description
| Metric | Description | How to read it | When to use | 
 -- | -- | -- | -- |
### Groundedness
Description: 
The measure evaluates how well the model's generated answers align with information from the input source. Even if the responses from LLM are factually correct, they will be considered ungrounded if they cannot be verified against the provided sources (such as your input source or your database).
How to read it: 
If the model's answers are highly grounded, it indicates that the facts covered in the AI system's responses are verifiable by the input source or internal database. On the other hand, low groundedness scores suggest low groundedness scores suggest that the facts mentioned in the AI system's responses may not be adequately supported or verifiable by the input source or internal database. In such cases, the model's generated answers could be based solely on its pre-trained knowledge, which may not align with the specific context or domain of the given input
Use it when: 
You are worried your application generates information that is not included as part of the LLM's trained knowledge (AKA unverifiable information).|



## Metrics configuration requirements
- GPT-Star metrics can be defined for any sequence-to-sequence task by adapting them to the task at hand. We define GPT-star metrics, which are metrics with ratings between 1-star to 5-star, on sequence-to-sequence tasks. You will need to use an advanced model version configured with specific evaluation instructions(prompt templates)

- The following inputs (data column names) are required to measure generation safety & quality: 
* **prompt (aka question) text** - the original prompt given by user
* **completion (aka answer) text** - the final completion from LLM API call that is returned to user
* **context text** - any context data that is sent to LLM API call together with original prompt. This can be configured through PromptFlow. (optional)
* **ground truth text** - the user-defined text as the source of truth (optional)
- other part is the context/desired results.  For example, if a user/customer hopes to get search results only from certain certified information sources/website, the user/customer can define in the evaluation steps. 

# Metric configuration requirements
| Metric | Description | When to use | Prompt  | Completion |  Context | Ground truth |
 -- | -- | -- | -- | -- | -- | -- |
| Coherence | -- | -- | Required | Required | -- |  -- |  
| Fluency | -- | -- | Required | Required | -- | -- |  
| Groundedness | -- | -- | Required | Required | Required | -- |  
| Relevance | -- | -- | Required | Required | Required | -- |  
| Similarity | -- | -- | Required | Required | -- | Required |  

> [!NOTE] Large language model evaluations are prone to the same bias that humans are. Always ensure there is a human-in-the-loop to view 

# Evaluation metrics

https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/how-to-bulk-test-evaluate-flow?view=azureml-api-2#understand-the-built-in-evaluation-metrics

# Getting started
Define GPT-4 endpoint for calculating hallucination (user pays the cost) 
    - Attach workspace UAI with access to the AOAI resource 
    - API connection in workspace (should be able to represent Batch endpoint) 
    - If no workspace connection, alert and provide prompt to guide users to create one 
    - Configuring permissions
Create an online endpoint (promptflow)	  
    - Specifies “flow outputs” to collect production data  (Prompt| Completion | Context  | Ground truth) 
    - Put into dataframe and log it, joining inputs and outputs  
    - Create PF deployment  
Create Model monitor 
    - Select deployed GPT4 LLM annotator model to create signal for groundedness  
    - Select target LLM output dataset (inputs & outputs ) 
    - Select metrics 
Consume/view groundedness metric in Studio UI 
    - View the metrics over time 
    - View histogram of distributions 
    - View samples of violations 