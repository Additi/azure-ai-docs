---
how-to-monitor-LLM-safety-quality---
title: Monitoring generative AI applications in production (preview)
titleSuffix: Azure Machine Learning
description: Monitor the safety and quality of generative AI deployed to production on Azure Machine Learning.
services: machine-learning
author: buchananwp
ms.author: wibuchan
ms.service: machine-learning
ms.subservice: mlops
ms.reviewer: mopeakande
reviewer: msakande
ms.topic: conceptual
ms.date: 09/06/2023
ms.custom: devplatv2
---

# Overview
Monitoring models in production is an essential part of the AI lifecycle. Changes in data and consumer behavior can influence your LLM application over time, resulting in outdated AI systems, which can produce undesired results that can negatively impact business outcomes and expose organizations to compliance and reputational risks. Azure Machine Learning now lets your organization monitor your LLM applications in production. 

You can collect production data using Model Data Collector(https://learn.microsoft.com/en-us/azure/machine-learning/concept-data-collection?view=azureml-api-2), analyze key safety & quality evaluation metrics (coherence/fluency/groundedness/relevance/similarity) on a recurring basis, receive timely alerts about critical issues, and visualize the results over time in a rich dashboard within the Azure Machine Learning studio.

For overall model monitoring basic concepts, please refer to the model monitoring concept article: https://learn.microsoft.com/en-us/azure/machine-learning/concept-model-monitoring?view=azureml-api-2

> [!IMPORTANT]
> Monitoring is currently in public preview. This preview is provided without a service-level agreement, and are not recommended for production workloads. Certain features might not be supported or might have constrained capabilities.
> For more information, see [Supplemental Terms of Use for Microsoft Azure Previews](https://azure.microsoft.com/support/legal/preview-supplemental-terms/).


# Prerequisites 
1. Required data items to flow level "inputs" aka "prompt" aka "question aka "outputs" aka "completion" aka "response"
1. Data collection: enable MDC at deployment step.
1. Workspace connection & UAI: You will need to configure a workspace connection for your Azure OpenAI resource to have appropriate permissions. Learn more here.
(aka.ms/uaiwcconfig)

# Supported evaluation model types
Generative AI monitoring uses the GPT family of models as evaluators for sequence-to-sequence tasks. This technique has shown encouraging empirical results and high correlation with human judgement when compared to standard evaluation metrics. These metrics are intended to be generated by state-of-the-art GPT language models: 
- GPT-3.5 Turbo
- GPT-4
- GPT-4-32k  

# Evaluation metrics 

## Metrics requirements
- The following inputs (data column names) are required to measure generation safety & quality: 
* **prompt (aka question) text** - the original prompt given by user
* **completion (aka answer) text** - the final completion from LLM API call that is returned to user
* **context text** - any context data that is sent to LLM API call together with original prompt. This can be configured through PromptFlow. (optional)
* **ground truth text** - the user-defined text as the source of truth (optional)
- other part is the context/desired results.  For example, if a user/customer hopes to get search results only from certain certified information sources/website, the user/customer can define in the evaluation steps. 

# Metric configuration
What parameters are configured in your data asset will dictate what metrics you can produce. 
| Metric | Prompt  | Completion |  Context | Ground truth |
| -- | -- | -- | -- | -- | 
| Coherence  | Required | Required | -- |  -- |  
| Fluency | Required | Required | -- | -- |  
| Groundedness | Required | Required | Required | -- |  
| Relevance | Required | Required | Required | -- |  
| Similarity | Required | Required | -- | Required |  


# Evaluation metrics


# Getting started
Define GPT-4 endpoint for calculating hallucination (user pays the cost) 
    - Attach workspace UAI with access to the AOAI resource 
    - API connection in workspace (should be able to represent Batch endpoint) 
    - If no workspace connection, alert and provide prompt to guide users to create one 
    - Configuring permissions
Create an online endpoint (promptflow)	  
    - Specifies “flow outputs” to collect production data  (Prompt| Completion | Context  | Ground truth) 
    - Put into dataframe and log it, joining inputs and outputs  
    - Create PF deployment  
Create Model monitor 
    - Select deployed GPT4 LLM annotator model to create signal for groundedness  
    - Select target LLM output dataset (inputs & outputs ) 
    - Select metrics 
Consume/view groundedness metric in Studio UI 
    - View the metrics over time 
    - View histogram of distributions 
    - View samples of violations 