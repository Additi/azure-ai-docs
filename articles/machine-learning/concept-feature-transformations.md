---
title: Feature transformations
titleSuffix: Azure Machine Learning
description: Learn about how Azure Machine Learning uses managed feature stores to create data transformation features and make these features available for training and deployment.
author: rsethur
ms.author: seramasu
ms.reviewer: franksolomon
ms.service: machine-learning
ms.subservice: mldata 
ms.topic: conceptual
ms.date: 05/23/2023  
---


# Feature transformations

[!INCLUDE [preview disclaimer](../../includes/machine-learning-preview-generic-disclaimer.md)]


In this article, you learn about feature transformation and how Azure Machine Learning supports it with a managed feature store.

Feature transformation involves modifying the features in a dataset to improve model performance. Feature transformation is done using transformation code, defined in a feature spec, to perform calculations on source data, allowing for the ability to develop and test transformations locally for faster experimentation.

## Defining a feature set
Feature sets are a collection of features generated by applying transformations on source system data. Feature sets encapsulate a source, the transformation function, and the materialization settings. A feature set is defined by a `FeatureSetSpec`. 

In the `FeatureSetSpec` definition, the following parts are relevant to feature transformation:
- `source`: defines the data location of the source table data and relevant metadata such as timestamp column in the table. Currently only time-series source and features are supported. Because of this, the `source.timestamp_column` property is mandatory.
- `feature_transformation_code`: defines the folder location of the feature transformer code.
- `features`: defines the feature schema generated by the feature transformer.
- `index_columns`: defines the index column(s) schema generated by the feature transformer.
- `source_lookback`: this property is used when the feature is doing aggregation on time-series (for example, window aggregation). The value of this property indicates the required time range of source data in the past for feature value at time T. Refer the [Best Practice](#Set-proper-source_lookback) section for more details.

:::image type="content" source="./media/concept-feature-transformations/featureset-spec-example.png" alt-text="Diagram mapping the parts of a FeatureSpec to example features in a dataframe":::

## Implement feature transformer for common types of transformations
<!-- add your content here -->

### Row-level Transformation

In row-level transformation, the calculation of a feature value on a row only uses column values of the current row.

We use the following source data as an example:

| user_id | timestamp | total_spend|
|---|---|---|
| 1 | 2022-12-19 06:00:00 | 12.00 |
| 2 | 2022-12-10 03:00:00 | 56.00 |
| 1 | 2022-12-25 13:00:00 | 112.00 |

And we define a new feature set, `user_total_spend_profile`:

```python
from pyspark.sql import Dataframe
from pyspark.ml import Transformer

class UserTotalSpendProfileTransformer(Transformer):

    def _transform(df: Dataframe) -> Dataframe:
        df.withColumn("is_high_spend_user", col("total_spend") > 100.0) \
           .withColumn("is_low_spend_user", col("total_spend") < 20.0)
```

This feature set `user_total_spend_profile` has three features:
- total_spend: double
- is_high_spend_user: bool
- is_low_spend_user: bool

And the calculated feature values are:
| user_id | timestamp | total_spend| is_high_spend_user | is_low_spend_user|
|---|---|---|---|---|
| 1 | 2022-12-19 06:00:00 | 12.00 | false| true |
| 2 | 2022-12-10 03:00:00 | 56.00 | false| false |
| 1 | 2022-12-25 13:00:00 | 112.00 | true | false |

### Sliding window aggregation
Sliding window aggregation is often used when a feature value presents statistics (for example, sum or average) accumulates over time. One common way to do that is to use the SparkSQL `Window` function, which defines a sliding window around each row in the data. 
 
The `Window` function can look into both the future and past of each row. In the context of machine learning features, we recommend defining `Window` to only look back in time from each row. For more information about preventing data leakage, see [Best Practice](#Prevent-data-leakage-in-feature-transformation).

We use the following source data as an example:

| user_id | timestamp | spend |
|---|---|---|
| 1 | 2022-12-10 06:00:00 | 12.00 |
| 2 | 2022-12-10 03:00:00 | 56.00 |
| 1 | 2022-12-11 01:00:00 | 10.00 |
| 2 | 2022-12-11 20:00:00 | 10.00 |
| 2 | 2022-12-12 02:00:00 | 100.00 |

And we define a new feature set, `user_rolling_spend`, which includes rolling 1 day and 3 day total spend of the user:

```python
from pyspark.sql import Dataframe
from pyspark.sql import functions as F
from pyspark.sql.window import Window
from pyspark.ml import Transformer

class UserRollingSpend(Transformer):

    def _transform(df: Dataframe) -> Dataframe:
        days = lambda i: i * 86400
        w_1d = (Window.partitionBy("user_id").orderBy(F.col("timestamp").cast('long'))\
                .rangeBetween(-days(1), 0))
        w_3d = (Window.partitionBy("user_id").orderBy(F.col("timestamp").cast('long')).\
                rangeBetween(-days(3), 0))
        res = df.withColumn("spend_1d_sum", F.sum("spend").over(w_1d)) \
            .withColumn("spend_3d_sum", F.sum("spend").over(w_3d)) \
            .select("user_id", "timestamp", "spend_1d_sum", "spend_3d_sum")
        return res
    
```

This feature set `user_rolling_spend` has two features:
- spend_1d_sum: double
- spend_3d_sum: double

And the calculated feature values are:
| user_id | timestamp | spend_1d_sum | spend_3d_sum |
|---|---|---|---|
| 1 | 2022-12-10 06:00:00 | 12.00 | 12.00 |
| 2 | 2022-12-10 03:00:00 | 56.00 | 56.00 |
| 1 | 2022-12-11 01:00:00 | 22.00 | 22.00 |
| 2 | 2022-12-11 20:00:00 | 10.00 | 66.00 |
| 2 | 2022-12-12 02:00:00 | 110.00 | 166.00 |

Each feature value is calculated using columns on the current row and the preceding rows within the range.

### Tumbling window aggregation

Another common way to aggregate data on time-series is to use tumbling window. The data is grouped into fix-size, nonoverlapping and continuous time windows, and then aggregated. For example, users can define daily or hourly features.

We recommend using `pyspark.sql.functions.window` functions to define tumbling window with consistent results. Additionally,  the "timestamp" of the output feature should align with the end of each tumbling window.

We use the following source data as an example:

| user_id | timestamp | spend |
|---|---|---|
| 1 | 2022-12-10 06:00:00 | 12.00 |
| 1 | 2022-12-10 16:00:00 | 10.00 |
| 2 | 2022-12-10 03:00:00 | 56.00 |
| 1 | 2022-12-11 01:00:00 | 10.00 |
| 2 | 2022-12-12 04:00:00 | 23.00 |
| 2 | 2022-12-12 12:00:00 | 10.00 |

And we define a new feature set, `user_daily_spend`:

```python
from pyspark.sql import functions as F
from pyspark.ml import Transformer
from pyspark.sql.dataframe import DataFrame

class TransactionFeatureTransformer(Transformer):
    def _transform(self, df: DataFrame) -> DataFrame:
        df1 = df.groupBy("user_id", F.window("timestamp", windowDuration="1 day",slideDuration="1 day"))\
                .agg(F.sum("spend").alias("daily_spend"))
        df2 = df1.select("user_id", df1.window.end.cast("timestamp").alias("end"),"daily_spend")
        df3 = df2.withColumn('timestamp', F.expr("end - INTERVAL 1 milliseconds")) \
              .select("user_id", "timestamp","daily_spend")
        return df3
```

This feature set `user_daily_spend` has two features:
- daily_spend: double

And the calculated feature values are:

| user_id | timestamp | daily_spend | 
|---|---|---|
| 1 | 2022-12-10 23:59:59 | 22.00 |
| 2 | 2022-12-10 23:59:59 | 56.00 |
| 1 | 2022-12-11 23:59:59 | 10.00 |
| 2 | 2022-12-12 23:59:59 | 33.00 |

### Stagger window aggregation

Stagger window aggregation is a minor variant of the tumbling window aggregation. The data is still grouped into fix-size windows. However, the windows can overlap with each other. This can be achieved using `pyspark.sql.functions.window`, with a `slideDuration` smaller than `windowDuration`. 

We use the following source data as an example:

| user_id | timestamp | spend |
|---|---|---|
| 1 | 2022-12-10 03:00:00 | 12.00 |
| 1 | 2022-12-10 09:00:00 | 10.00 |
| 1 | 2022-12-11 05:00:00 | 8.00 |
| 2 | 2022-12-12 14:00:00 | 56.00 |

And we define a new feature set, `user_sliding_24hr_spend`:

```python
from pyspark.sql import functions as F
from pyspark.ml import Transformer
from pyspark.sql.dataframe import DataFrame

class TrsactionFeatureTransformer(Transformer):
    def _transform(self, df: DataFrame) -> DataFrame:
        df1 = df.groupBy("user_id", F.window("timestamp", windowDuration="1 day",slideDuration="6 hours"))\
                .agg(F.sum("spend").alias("sliding_24hr_spend"))
        df2 = df1.select("user_id", df1.window.end.cast("timestamp").alias("end"),"sliding_24hr_spend")
        df3 = df2.withColumn('timestamp', F.expr("end - INTERVAL 1 milliseconds")) \
              .select("user_id", "timestamp","sliding_24hr_spend")
        return df3
```

This feature set `user_sliding_24hr_spend` has one feature:
- sliding_24hr_spend: double

And the calculated feature values are:

| user_id | timestamp | sliding_24hr_spend | 
|---|---|---|
| 1 | 2022-12-10 05:59:59 | 12.00 |
| 1 | 2022-12-10 11:59:59 | 22.00 |
| 1 | 2022-12-10 17:59:59 | 22.00 |
| 1 | 2022-12-10 23:59:59 | 22.00 |
| 1 | 2022-12-11 05:59:59 | 18.00 |
| 1 | 2022-12-11 11:59:59 | 8.00 |
| 1 | 2022-12-11 17:59:59 | 8.00 |
| 1 | 2022-12-11 13:59:59 | 8.00 |
| 1 | 2022-12-11 05:59:59 | 18.00 |
| 2 | 2022-12-12 17:59:59 | 56.00 |
| 2 | 2022-12-12 23:59:59 | 56.00 |
| 2 | 2022-12-13 05:59:59 | 56.00 |
| 2 | 2022-12-13 11:59:59 | 56.00 |

## Best Practices in Defining Feature transformation

### How are features calculated

Usually features are calculated on a given feature time window, e.g.:
- User investigates feature values by invoking `featureSetSpec.to_spark_dataframe(feature_window_start_ts, feature_window_end_ts)`. 
- Recurrent feature materialization jobs materialize feature values in an incremental way. Each job processes a feature window.
- User requests to backfill (materialization) feature values of a feature window in the past to prepare for offline training.

Given a feature window, feature_window_start_ts/feature_window_end_ts, Feature(set) calculation can be abstracted into the following steps:
- read data from the source data table within the time range `[feature_window_start_ts - source_lookback, feature_window_end_ts)`. Inclusive on the start of the window and exclusive on the end of the window.
- apply the feature transformer on the data and get the calculated feature(set)
- filter the feature(set) to include only feature records within the feature window `[feature_window_start_ts, feature_window_end_ts)`

Here's a sample code snippet that provides intuition on how feature store APIs computes the features:

```python
# define the source data time window according to feature window
source_window_start_ts = feature_window_start_ts - source_lookback
source_window_end_ts = feature_window_end_ts

# read source table into a dataframe
df1 = spark.read.parquet(source.path).filter(df1["timestamp"] >= source_window_start_ts && df1["timestamp"] < source_window_end_ts)

# apply the feature transformer
df2 = FeatureTransformer._transform(df1)

## filter the feature(set) to include only feature records within the feature window
feature_set_df = df2.filter(df2["timestamp"] >= feature_window_start_ts && df2["timestamp"] < feature_window_end_ts)
```

The diagram shows the relationship between the source data window and feature window in the feature(set) calculation.

:::image type="content" source="./media/concept-feature-transformations/feature-calculation.png" alt-text="Diagram mapping the parts of a FeatureSpec to example features in a dataframe":::


### Prevent data leakage in feature transformation

For each calculated feature value, if its timestamp value is `ts_0`, it should be calculated using data in `source` that have timestamp on or before `ts_0` only. This avoids data leakage caused by feature calculation using data from the future of the feature event time.

Such leakage happens usually when using sliding/tumbling/stagger window aggregation. To avoid leakage, the best practices are:
- sliding window aggregation: define Window to only look back in time from each row
- tumbling/stagger window aggregation: define the feature timestamp using the end of each window

Here are some good/bad examples for each case.

| aggregation | good example | bad example with data leakage | 
|---|---|---|
|sliding window | Window.partitionBy("user_id")<br>.orderBy(F.col("timestamp").cast('long'))<br>.`rangeBetween(-days(4), 0)` | Window.partitionBy("user_id")<br>.orderBy(F.col("timestamp").cast('long'))<br>.`rangeBetween(-days(2), days(2))` |
|tumbling/stagger window | df1 = df.groupBy("user_id", F.window("timestamp", windowDuration="1 day",slideDuration="1 day"))<br>.agg(F.sum("spend").alias("daily_spend"))<br><br>df2 = df1.select("user_id", df1.window.`end`.cast("timestamp").alias("timestamp"),"daily_spend") | df1 = df.groupBy("user_id", F.window("timestamp", windowDuration="1 day",slideDuration="1 day"))<br>.agg(F.sum("spend").alias("daily_spend"))<br><br>df2 = df1.select("user_id", df1.window.`start`.cast("timestamp").alias("timestamp"),"daily_spend")  |

Data leakage in feature transformation definition can cause issues:
- Incorrectness in the calculated/materialized feature values
- Inconsistency in get_offline_feature when using the materialized feature value vs calculate on the fly.

### Set proper source_lookback

In aggregations on time-series (sliding/tumbling/stagger window aggregation), user needs to set `source_lookback` properly. 

`source_lookback` should be defined to a time delta value that presents the range of source data needed for a feature value of a given timestamp.

Here are the recommended `source_lookback` values for the common types of transformations:

| transformation type | `source_lookback` |
|---|---|
| row-level transformation | 0 (default) |
| aggregation on non time series dimension (non time-series features) | 0 (default) |
| sliding window | size of the largest window range in the transformer.<br> e.g.<br> source_lookback = 3days when the feature set defines 3 day rolling features <br> source_lookback = 7 days when the feature set defines both 3 day and 7 day rolling features |
| tumbling/stagger window | value of windowDuration in window definition. for example, source_lookback = 1day when using `window("timestamp", windowDuration="1 day",slideDuration="6 hours)` |

When `source_lookback` isn't set correctly, it can cause:
- Incorrectness in the calculated/materialized feature values



## Next steps
<!-- Add a context sentence for the following links -->
- [Write concepts](contribute-how-to-write-concept.md)
- [Links](links-how-to.md)


